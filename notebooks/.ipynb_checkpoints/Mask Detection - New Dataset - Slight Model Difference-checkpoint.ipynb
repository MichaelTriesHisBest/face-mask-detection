{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T20:48:30.960302Z",
     "start_time": "2020-11-15T20:48:30.905918Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# our nice list of imports is in here\n",
    "#i hate this, but the notebook won't recognize our custom modules w/o it\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import lib\n",
    "from lib import utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "import random\n",
    "from shutil import copyfile, move\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.models import Sequential        # keras is only compatible with python 3.6 or lower\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "\n",
    "from skimage import io, data\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.transform import rescale\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Mask Detection\n",
    "\n",
    "\n",
    "### Brief Overview\n",
    "\n",
    "Simply put, our goal is to create a neural network that will (1) identify face masks or lack thereof on a human face, and (2) determine whether that facemask is correctly worn.\n",
    "\n",
    "\n",
    "Luckily for us, Kaggle had a contest earlier this year related to (1) -- so a dataset was readily available, and it has proven to be quite effective at training a mask/no-mask neural net with our testing images. (Kaggle)\n",
    "\n",
    "\n",
    "However, the dataset is extremely lacking when it comes to incorrect mask usage.  We did locate an additional dataset that consists only of faces with correctly worn masks and incorrectly worn masks. (CMFD/IMFD)\n",
    "\n",
    "### Data\n",
    "\n",
    "1. Training Data - We have training data from both datasets.  The images in each set have been pre-processed with relevant data written to CSV files to ease the process load.\n",
    "  + Kaggle Dataset -- \n",
    "2. Testing Data - Our testing data consists of \n",
    "\n",
    "\n",
    "### Models\n",
    "\n",
    "To that end, we need a few different models:\n",
    "1. One to predict whether there's a mask on a face or no mask\n",
    "  + For this, we will use the Kaggle Dataset, and focus on the two largest features available in it: whether there is a face mask or not.\n",
    "  + Potentially, we could add the data from the CMFD/IMFD dataset as representations of masks for the training set, since we'll be determining whether they're correct or incorrectly worn in the next stage.  The only concern there is a dataset that's very, very heavy on masks vs. no masks\n",
    "2. One to predict whether, given a mask is on a face, it is worn correctly\n",
    "  + For this, we will use the CMFD/IMFD dataset.  There are only two features to this dataset: correctly worn and incorrectly worn face masks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T20:48:31.023146Z",
     "start_time": "2020-11-15T20:48:30.962962Z"
    }
   },
   "outputs": [],
   "source": [
    "# define directory paths for easier navigation\n",
    "NOTEBOOK_DIR = !pwd\n",
    "NOTEBOOK_DIR = NOTEBOOK_DIR[0]\n",
    "ROOT_DIR = os.path.abspath(os.path.join(NOTEBOOK_DIR, os.pardir))\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'images')\n",
    "IMG_DATA_DIR = os.path.join(DATA_DIR, 'img_data')\n",
    "KAGGLE_DIR = os.path.join(IMAGES_DIR, 'kaggle')\n",
    "HC_DATA_DIR = os.path.join(DATA_DIR, 'haar_cascades')\n",
    "\n",
    "# MNM_TRIAL_DIR = os.path.join(IMAGES_DIR, 'mnm_trial')\n",
    "# MNM_TRAINING = os.path.join(MNM_TRIAL_DIR, 'training')\n",
    "# MNM_TESTING = os.path.join(MNM_TRIAL_DIR, 'testing')\n",
    "\n",
    "PRAJNA_SRC_DIR = os.path.join(DATA_DIR, 'prajna-dataset')\n",
    "PMASK_SRC = os.path.join(PRAJNA_SRC_DIR, 'mask')\n",
    "PNOMASK_SRC = os.path.join(PRAJNA_SRC_DIR, 'nomask')\n",
    "\n",
    "PTRAINING_DIR = os.path.join(PRAJNA_SRC_DIR, 'dest/train')\n",
    "PTESTING_DIR = os.path.join(PRAJNA_SRC_DIR, 'dest/test')\n",
    "PVAL_DIR = os.path.join(PRAJNA_SRC_DIR, 'dest/val')\n",
    "\n",
    "PTRAINING_MASK = os.path.join(PRAJNA_SRC_DIR, 'dest/train/mask')\n",
    "PTESTING_MASK = os.path.join(PRAJNA_SRC_DIR, 'dest/test/mask')\n",
    "PVAL_MASK = os.path.join(PRAJNA_SRC_DIR, 'dest/val/mask')\n",
    "\n",
    "PTRAINING_NOMASK = os.path.join(PRAJNA_SRC_DIR, 'dest/train/nomask')\n",
    "PTESTING_NOMASK = os.path.join(PRAJNA_SRC_DIR, 'dest/test/nomask')\n",
    "PVAL_NOMASK = os.path.join(PRAJNA_SRC_DIR, 'dest/val/nomask')\n",
    "\n",
    "\n",
    "EXTENDED_SRC_DIR = os.path.join(DATA_DIR, 'extended-dataset')\n",
    "EMASK_SRC = os.path.join(EXTENDED_SRC_DIR, 'mask')\n",
    "ENOMASK_SRC = os.path.join(EXTENDED_SRC_DIR, 'nomask')\n",
    "\n",
    "ETRAINING_DIR = os.path.join(EXTENDED_SRC_DIR, 'dest/train')\n",
    "ETESTING_DIR = os.path.join(EXTENDED_SRC_DIR, 'dest/test')\n",
    "EVAL_DIR = os.path.join(EXTENDED_SRC_DIR, 'dest/val')\n",
    "\n",
    "ETRAINING_MASK = os.path.join(EXTENDED_SRC_DIR, 'dest/train/mask')\n",
    "ETESTING_MASK = os.path.join(EXTENDED_SRC_DIR, 'dest/test/mask')\n",
    "EVAL_MASK = os.path.join(EXTENDED_SRC_DIR, 'dest/val/mask')\n",
    "\n",
    "ETRAINING_NOMASK = os.path.join(EXTENDED_SRC_DIR, 'dest/train/nomask')\n",
    "ETESTING_NOMASK = os.path.join(EXTENDED_SRC_DIR, 'dest/test/nomask')\n",
    "EVAL_NOMASK = os.path.join(EXTENDED_SRC_DIR, 'dest/val/nomask')\n",
    "\n",
    "\n",
    "SRC_DIR = os.path.join(DATA_DIR, 'face_mask_data')\n",
    "MASK_SRC = os.path.join(SRC_DIR, 'mask')\n",
    "NOMASK_SRC = os.path.join(SRC_DIR, 'nomask')\n",
    "\n",
    "TRAINING_DIR = os.path.join(SRC_DIR, 'dest/train')\n",
    "TESTING_DIR = os.path.join(SRC_DIR, 'dest/test')\n",
    "VAL_DIR = os.path.join(SRC_DIR, 'dest/val')\n",
    "\n",
    "TRAINING_MASK = os.path.join(SRC_DIR, 'dest/train/mask')\n",
    "TESTING_MASK = os.path.join(SRC_DIR, 'dest/test/mask')\n",
    "VAL_MASK = os.path.join(SRC_DIR, 'dest/val/mask')\n",
    "\n",
    "TRAINING_NOMASK = os.path.join(SRC_DIR, 'dest/train/nomask')\n",
    "TESTING_NOMASK = os.path.join(SRC_DIR, 'dest/test/nomask')\n",
    "VAL_NOMASK = os.path.join(SRC_DIR, 'dest/val/nomask')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MASK_SRC_DIR=os.path.join(MNM_TRIAL_DIR, 'mask')\n",
    "# TRAINING_MASK=os.path.join(MNM_TRAINING, 'mask')\n",
    "# TESTING_MASK=os.path.join(MNM_TESTING, 'mask')\n",
    "\n",
    "# NOMASK_SRC_DIR=os.path.join(MNM_TRIAL_DIR, 'nomask')\n",
    "# TRAINING_NOMASK=os.path.join(MNM_TRAINING, 'nomask')\n",
    "# TESTING_NOMASK=os.path.join(MNM_TESTING, 'nomask')\n",
    "\n",
    "KAGGLE_IMAGES_DIR = \"https://github.com/brtonnies/face-mask-detection/blob/data-branch/data/images\"\n",
    "# CMFD_IMAGES_DIR = \"https://github.com/brtonnies/face-mask-detection/blob/data-branch/data/images/CMFD/images\"\n",
    "# IMFD_IMAGES_DIR = \"https://github.com/brtonnies/face-mask-detection/blob/data-branch/data/images/IMFD/images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Mask vs. No Mask (MNM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:06.319969Z",
     "start_time": "2020-11-12T20:23:06.317684Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:06.660624Z",
     "start_time": "2020-11-12T20:23:06.658847Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparations\n",
    "\n",
    "+ We keep pre-compiled versions of our different models on hand (again, to reduce process/load times)\n",
    "+ Unless the training data changes, we default to using those existing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:08.127649Z",
     "start_time": "2020-11-12T20:23:08.125432Z"
    }
   },
   "source": [
    "1. First model used ONLY Prajna Dataset (97% acc)\n",
    "2. Second model is training now, with additional 'no mask' images from Kaggle\n",
    "3. Third will have additional 'mask' images from kaggle\n",
    "\n",
    "**DON'T FORGET TO REVERT IMAGES TO BEST MODEL VERSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:08.590623Z",
     "start_time": "2020-11-12T20:23:08.588383Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T20:48:31.030075Z",
     "start_time": "2020-11-15T20:48:31.026468Z"
    }
   },
   "outputs": [],
   "source": [
    "# def data_summary(main_path):\n",
    "    \n",
    "#     yes_path = main_path+'/mask'\n",
    "#     no_path = main_path+'/nomask'\n",
    "        \n",
    "#     # number of files (images) that are in the the folder named 'yes' that represent tumorous (positive) examples\n",
    "#     m_pos = len(os.listdir(yes_path))\n",
    "#     # number of files (images) that are in the the folder named 'no' that represent non-tumorous (negative) examples\n",
    "#     m_neg = len(os.listdir(no_path))\n",
    "#     # number of all examples\n",
    "#     m = (m_pos+m_neg)\n",
    "    \n",
    "#     pos_prec = (m_pos* 100.0)/ m\n",
    "#     neg_prec = (m_neg* 100.0)/ m\n",
    "    \n",
    "#     print(f\"Number of examples: {m}\")\n",
    "#     print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\") \n",
    "#     print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\") \n",
    "    \n",
    "# augmented_data_path = os.path.join(EXTENDED_SRC_DIR, 'dest')\n",
    "# data_summary(augmented_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T20:48:32.733268Z",
     "start_time": "2020-11-15T20:48:31.033206Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(src, train, test, split_size):\n",
    "    dataset = []\n",
    "    \n",
    "    for unitData in os.listdir(src):\n",
    "        data = os.path.join(src, unitData)\n",
    "        if(os.path.getsize(data) > 0 and os.path.isfile(data)):\n",
    "            dataset.append(unitData)\n",
    "        else:\n",
    "            print('Skipped ' + unitData)\n",
    "            print('Invalid file i.e zero size')\n",
    "    \n",
    "    train_set_length = int(len(dataset) * split_size)\n",
    "    test_set_length = int(len(dataset) - train_set_length)\n",
    "    shuffled_set = random.sample(dataset, len(dataset))\n",
    "    train_set = dataset[0:train_set_length]\n",
    "    test_set = dataset[-test_set_length:]\n",
    "       \n",
    "    for unitData in train_set:\n",
    "        temp_train_set = os.path.join(src, unitData)\n",
    "        final_train_set = os.path.join(train, unitData)\n",
    "        \n",
    "        copyfile(temp_train_set, final_train_set)\n",
    "    \n",
    "    for unitData in test_set:\n",
    "        temp_test_set = os.path.join(src, unitData)\n",
    "        final_test_set = os.path.join(test, unitData)\n",
    "        copyfile(temp_test_set, final_test_set)\n",
    "\n",
    "        \n",
    "split_size = 0.8\n",
    "split_data(MASK_SRC, TRAINING_MASK, TESTING_MASK, split_size)\n",
    "split_data(NOMASK_SRC, TRAINING_NOMASK, TESTING_NOMASK, split_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:11.386097Z",
     "start_time": "2020-11-12T20:23:11.384123Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:11.807979Z",
     "start_time": "2020-11-12T20:23:11.805661Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-11T19:57:01.408Z"
    }
   },
   "source": [
    "## Keras Sequential Model\n",
    "\n",
    "#### Building the Model\n",
    "In this proposed method, the Face Mask detection model is built using the Sequential API of the keras library. This allows us to create the new layers for our model step by step. The various layers used for our CNN model is described below.\n",
    "\n",
    "1. The first layer is Conv2D with **100 kernels** of size $3\\times3$. \n",
    "  + The activation function is ‘ReLu’.  \n",
    "  + The input size is set to $150\\times 150 \\times 3$ for all the images to be trained and tested\n",
    "\n",
    "2. MaxPooling2D is used for the second layer with pool size of $2 \\times 2$.\n",
    "\n",
    "3. The next layer is another Conv2D layer \n",
    "  + Has its own 100 kernels of the same size: $3\\times3$ \n",
    "  + Activation function is ‘ReLu’. \n",
    "  + The layer is followed by another MaxPooling2D layer with pool size $2 \\times 2$.\n",
    "\n",
    "4. Then the Flatten() layer to flatten *all* the layers into a single dimension.\n",
    "\n",
    "5. **NEW:** After the Flatten layer, we added dropout (0.5) layer to prevent the model from overfitting -- which was killing us in early trials.\n",
    "\n",
    "6. Then we used the Dense layer \n",
    "  + 50 'units' \n",
    "  + Activation function: ‘ReLu’.\n",
    "\n",
    "7. The last layer of our model will be another Dense Layer, with only two units and activation function ‘Softmax’. The softmax function outputs a vector representing the probability distributions of each of the input units. Here, two input units are used. The softmax function will output a vector with two probability distribution values: is there a mask on this face or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T20:48:33.180051Z",
     "start_time": "2020-11-15T20:48:32.739865Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "kernel_size = (3, 3)\n",
    "image_shape = (150, 150, 3)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(200, kernel_size, activation='relu', input_shape=image_shape, strides=2),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "#     tf.keras.layers.Conv2D(100, (3,3), activation='tanh'),\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(100, kernel_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),   # previously we were way overfitting, so we added dropout\n",
    "    \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax'),\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-4, epsilon=1e-3)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Training/Testing/Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T20:48:33.492022Z",
     "start_time": "2020-11-15T20:48:33.183875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1680 images belonging to 2 classes.\n",
      "Found 421 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(ETRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(150, 150))\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(ETESTING_DIR, \n",
    "                                                              batch_size=10, \n",
    "                                                              target_size=(150, 150))\n",
    "\n",
    "# (1) train the all data extended model\n",
    "# (2) re-train prajna, prajna-kaggle-mask to save new checkpoints (and load them :D)\n",
    "# (3) also try and do one model where only new masks are added to the set\n",
    "\n",
    "# model_path = '../models/mnm/prajna'\n",
    "# checkpoint_path = '../models/mnm/prajna.checkpoint' \n",
    "\n",
    "model_path = '../models/mnm/new_dataset_tanh'\n",
    "checkpoint_path = '../models/mnm/new_dataset_tanh.checkpoint'\n",
    "\n",
    "# model_path = '../models/mnm/prajna-kaggle-nomask-extend'\n",
    "# checkpoint_path = '../models/mnm/prajna-kaggle-nomask-extend-{epoch:03d}.model'\n",
    "# model_path = '../models/mnm/prajna-kaggle-mask-extend'\n",
    "# checkpoint_path = '../models/mnm/prajna-kaggle-mask-extend-{epoch:03d}.model'\n",
    "# model_path = '../models/mnm/prajna-kaggle-all-extend'\n",
    "# checkpoint_path = '../models/mnm/prajna-kaggle-all-extend-{epoch:03d}.model'\n",
    "\n",
    "\n",
    "# NOTE: here's something cool - we can use checkpoints\n",
    "#       to save/load the weights from previous model training\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     model.load_weights(os.path.join(checkpoint_path, 'variables/variables'))\n",
    "    \n",
    "# alternatively, we can load the whole model from the checkpoint state\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model = load_model(checkpoint_path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path, \n",
    "    monitor = 'val_loss', \n",
    "    verbose = 1, \n",
    "    save_best_only = True,\n",
    "    mode = 'auto'\n",
    ")\n",
    "\n",
    "logger = CSVLogger(os.path.join(model_path, 'logs/training.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-15T20:48:30.994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.7216\n",
      "Epoch 00001: val_loss improved from inf to 0.32771, saving model to ../models/mnm/new_dataset_tanh.checkpoint\n",
      "WARNING:tensorflow:From /Users/brtonnies/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/new_dataset_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 152s 907ms/step - loss: 0.5629 - acc: 0.7208 - val_loss: 0.3277 - val_acc: 0.8836\n",
      "Epoch 2/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.4851 - acc: 0.7898\n",
      "Epoch 00002: val_loss did not improve from 0.32771\n",
      "168/168 [==============================] - 150s 892ms/step - loss: 0.4834 - acc: 0.7905 - val_loss: 0.3940 - val_acc: 0.8100\n",
      "Epoch 3/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.4679 - acc: 0.7838\n",
      "Epoch 00003: val_loss did not improve from 0.32771\n",
      "168/168 [==============================] - 172s 1s/step - loss: 0.4663 - acc: 0.7839 - val_loss: 0.3721 - val_acc: 0.8527\n",
      "Epoch 4/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.4371 - acc: 0.8114\n",
      "Epoch 00004: val_loss improved from 0.32771 to 0.26317, saving model to ../models/mnm/new_dataset_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/new_dataset_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 164s 973ms/step - loss: 0.4369 - acc: 0.8113 - val_loss: 0.2632 - val_acc: 0.8907\n",
      "Epoch 5/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8365\n",
      "Epoch 00005: val_loss did not improve from 0.26317\n",
      "168/168 [==============================] - 163s 971ms/step - loss: 0.3915 - acc: 0.8369 - val_loss: 0.2814 - val_acc: 0.8812\n",
      "Epoch 6/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8455\n",
      "Epoch 00006: val_loss improved from 0.26317 to 0.25232, saving model to ../models/mnm/new_dataset_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/new_dataset_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 158s 943ms/step - loss: 0.3770 - acc: 0.8458 - val_loss: 0.2523 - val_acc: 0.8955\n",
      "Epoch 7/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8461\n",
      "Epoch 00007: val_loss did not improve from 0.25232\n",
      "168/168 [==============================] - 153s 914ms/step - loss: 0.3563 - acc: 0.8464 - val_loss: 0.2569 - val_acc: 0.8812\n",
      "Epoch 8/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8617\n",
      "Epoch 00008: val_loss improved from 0.25232 to 0.21665, saving model to ../models/mnm/new_dataset_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/new_dataset_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 161s 959ms/step - loss: 0.3396 - acc: 0.8619 - val_loss: 0.2166 - val_acc: 0.9145\n",
      "Epoch 9/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.3144 - acc: 0.8778\n",
      "Epoch 00009: val_loss improved from 0.21665 to 0.20804, saving model to ../models/mnm/new_dataset_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/new_dataset_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 160s 950ms/step - loss: 0.3138 - acc: 0.8780 - val_loss: 0.2080 - val_acc: 0.9026\n",
      "Epoch 10/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.8946\n",
      "Epoch 00010: val_loss did not improve from 0.20804\n",
      "168/168 [==============================] - 157s 934ms/step - loss: 0.2706 - acc: 0.8946 - val_loss: 0.2339 - val_acc: 0.9169\n",
      "Epoch 11/30\n",
      "167/168 [============================>.] - ETA: 1s - loss: 0.3101 - acc: 0.8790\n",
      "Epoch 00011: val_loss improved from 0.20804 to 0.17801, saving model to ../models/mnm/new_dataset_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/new_dataset_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 244s 1s/step - loss: 0.3093 - acc: 0.8798 - val_loss: 0.1780 - val_acc: 0.9335\n",
      "Epoch 12/30\n",
      "167/168 [============================>.] - ETA: 1s - loss: 0.3035 - acc: 0.8731\n",
      "Epoch 00012: val_loss did not improve from 0.17801\n",
      "168/168 [==============================] - 227s 1s/step - loss: 0.3037 - acc: 0.8726 - val_loss: 0.1788 - val_acc: 0.9335\n",
      "Epoch 13/30\n",
      "167/168 [============================>.] - ETA: 1s - loss: 0.2856 - acc: 0.8892\n",
      "Epoch 00013: val_loss improved from 0.17801 to 0.16227, saving model to ../models/mnm/new_dataset_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/new_dataset_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 225s 1s/step - loss: 0.2846 - acc: 0.8899 - val_loss: 0.1623 - val_acc: 0.9430\n",
      "Epoch 14/30\n",
      "167/168 [============================>.] - ETA: 1s - loss: 0.2798 - acc: 0.8880\n",
      "Epoch 00014: val_loss did not improve from 0.16227\n",
      "168/168 [==============================] - 206s 1s/step - loss: 0.2796 - acc: 0.8881 - val_loss: 0.1830 - val_acc: 0.9382\n",
      "Epoch 15/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.8922\n",
      "Epoch 00015: val_loss did not improve from 0.16227\n",
      "168/168 [==============================] - 124s 737ms/step - loss: 0.2833 - acc: 0.8917 - val_loss: 0.2039 - val_acc: 0.9169\n",
      "Epoch 16/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.8976\n",
      "Epoch 00016: val_loss did not improve from 0.16227\n",
      "168/168 [==============================] - 93s 554ms/step - loss: 0.2623 - acc: 0.8976 - val_loss: 0.1900 - val_acc: 0.9050\n",
      "Epoch 17/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.8982\n",
      "Epoch 00017: val_loss improved from 0.16227 to 0.16225, saving model to ../models/mnm/new_dataset_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/new_dataset_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 146s 869ms/step - loss: 0.2617 - acc: 0.8982 - val_loss: 0.1623 - val_acc: 0.9430\n",
      "Epoch 18/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2607 - acc: 0.8970\n",
      "Epoch 00018: val_loss did not improve from 0.16225\n",
      "168/168 [==============================] - 127s 753ms/step - loss: 0.2598 - acc: 0.8976 - val_loss: 0.1701 - val_acc: 0.9454\n",
      "Epoch 19/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.8970\n",
      "Epoch 00019: val_loss did not improve from 0.16225\n",
      "168/168 [==============================] - 136s 809ms/step - loss: 0.2647 - acc: 0.8970 - val_loss: 0.1765 - val_acc: 0.9359\n",
      "Epoch 20/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.8994\n",
      "Epoch 00020: val_loss did not improve from 0.16225\n",
      "168/168 [==============================] - 123s 733ms/step - loss: 0.2667 - acc: 0.8994 - val_loss: 0.1653 - val_acc: 0.9359\n",
      "Epoch 21/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9096\n",
      "Epoch 00021: val_loss did not improve from 0.16225\n",
      "168/168 [==============================] - 148s 882ms/step - loss: 0.2379 - acc: 0.9095 - val_loss: 0.1645 - val_acc: 0.9525\n",
      "Epoch 22/30\n",
      "167/168 [============================>.] - ETA: 1s - loss: 0.2435 - acc: 0.9138\n",
      "Epoch 00022: val_loss improved from 0.16225 to 0.15015, saving model to ../models/mnm/new_dataset_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/new_dataset_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 187s 1s/step - loss: 0.2466 - acc: 0.9131 - val_loss: 0.1501 - val_acc: 0.9477\n",
      "Epoch 23/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9060\n",
      "Epoch 00023: val_loss did not improve from 0.15015\n",
      "168/168 [==============================] - 134s 796ms/step - loss: 0.2389 - acc: 0.9065 - val_loss: 0.1534 - val_acc: 0.9382\n",
      "Epoch 24/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.9042\n",
      "Epoch 00024: val_loss did not improve from 0.15015\n",
      "168/168 [==============================] - 128s 760ms/step - loss: 0.2401 - acc: 0.9042 - val_loss: 0.1589 - val_acc: 0.9382\n",
      "Epoch 25/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9114\n",
      "Epoch 00025: val_loss improved from 0.15015 to 0.14786, saving model to ../models/mnm/new_dataset_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/new_dataset_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 138s 822ms/step - loss: 0.2415 - acc: 0.9119 - val_loss: 0.1479 - val_acc: 0.9382\n",
      "Epoch 26/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2473 - acc: 0.9096\n",
      "Epoch 00026: val_loss improved from 0.14786 to 0.12875, saving model to ../models/mnm/new_dataset_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/new_dataset_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 134s 799ms/step - loss: 0.2470 - acc: 0.9095 - val_loss: 0.1287 - val_acc: 0.9549\n",
      "Epoch 27/30\n",
      " 30/168 [====>.........................] - ETA: 2:30 - loss: 0.2740 - acc: 0.9100"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=30,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint, logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Did We Do?\n",
    "\n",
    "To answer that, we have to answer a few other questions:\n",
    "1. What do the training/validation results look like?\n",
    "  + Can we accurately predict the data in the dataset source images?\n",
    "2. How well does our model predict a completely different dataset?\n",
    "  + For this, we'll use the training set from Kaggle so we can also check our accuracy\n",
    "3. How well does our model predict in a different setting?\n",
    "  + We'll use a live WebCam feed or video file and have our model predict in real time (or close to it -- framerate is spotty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-15T20:48:31.005Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_accuracy_loss(history):\n",
    "    \"\"\"\n",
    "        Plot the accuracy and the loss during the training of the nn.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(18,8))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(221)\n",
    "    plt.plot(history.history['acc'],'bo--', label = \"acc\")\n",
    "    plt.plot(history.history['val_acc'], 'ro--', label = \"val_acc\")\n",
    "    plt.title(\"train_acc vs val_acc\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss function\n",
    "    plt.subplot(222)\n",
    "    plt.plot(history.history['loss'],'bo--', label = \"loss\")\n",
    "    plt.plot(history.history['val_loss'], 'ro--', label = \"val_loss\")\n",
    "    plt.title(\"train_loss vs val_loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_accuracy_loss(history)\n",
    "# history.history\n",
    "# model.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting The Current Dataset\n",
    "\n",
    "+ We'll start with the images in the source directory of masks\n",
    "  1. First we go through the images, use facial detection to find bounding boxes and/or keypoints\n",
    "  2. Then we  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-15T20:48:31.013Z"
    }
   },
   "outputs": [],
   "source": [
    "len(os.listdir(PMASK_SRC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Another Dataset\n",
    "\n",
    "Our model's performance in the training/validation stage was admirable.  Let's give it some images from another dataset and see how it performs with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-15T20:48:31.022Z"
    }
   },
   "outputs": [],
   "source": [
    "# we'll be using the Kaggle mask detection contest training dataset\n",
    "# df = pd.read_csv(os.path.join(IMG_DATA_DIR, 'kaggle_training.csv'), index_col=0)\n",
    "\n",
    "# # the kaggle dataset actually had errors in its labeling -- on purpose\n",
    "# df.rename(columns={'x1': 'x', 'x2': 'y', 'y1': 'w', 'y2': 'h'}, inplace=True)\n",
    "# df.drop(['bbox'], axis=1, inplace=True)\n",
    "\n",
    "# df = df[df['classname'].isin(['face_with_mask', 'face_with_mask_incorrect', 'face_no_mask'])]\n",
    "\n",
    "\n",
    "# # df['pred'] = np.nan\n",
    "                      \n",
    "# test_images = list(df['name'].unique())\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-15T20:48:31.024Z"
    }
   },
   "outputs": [],
   "source": [
    "# labels_dict={ 0:'No Mask', 1:'Mask' }\n",
    "# color_dict={ 0: 'magenta', 1: 'cyan' }\n",
    "\n",
    "# # name = test_images[0]\n",
    "# total = len(test_images)\n",
    "# counter = 1\n",
    "# idx = 0\n",
    "# for name in test_images:\n",
    "#     # fetch the image from the remote repository\n",
    "#     url = os.path.join(KAGGLE_IMAGES_DIR, \"{}?raw=true\".format(name))\n",
    "#     resp = urllib.request.urlopen(url)\n",
    "#     image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "#     image = cv2.imdecode(image, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "#     # locate the image within the dataframe so we can find bounding boxes\n",
    "#     # NOTE: bounding boxes were pre-detected in this dataset and stored in the dataframe CSV\n",
    "#     to_test = df[df['name'] == name]\n",
    "#     #     print(to_test)\n",
    "\n",
    "\n",
    "# #     fig, ax = plt.subplots(1)\n",
    "# #     ax.imshow(image, cmap='gray')\n",
    "\n",
    "#     preds = list()\n",
    "#     # print([i for i in list(faces) if len(i) > 4])\n",
    "#     for i in range(len(to_test)):\n",
    "#         print(\"Index: {}\".format(i+idx))\n",
    "#         im = df.iloc[i+idx]\n",
    "\n",
    "#         face_region = image[im['y']:im['y']+im['h'], im['x']:im['x']+im['w']]\n",
    "#         if face_region.size > 0:\n",
    "#             resized = cv2.resize(face_region, (450, 450))\n",
    "#             normalized = resized/255.0\n",
    "#             reshaped = np.reshape(normalized, (-1, 150, 150, 3))\n",
    "#             reshaped = np.vstack([reshaped])\n",
    "#             pred = model.predict(reshaped)\n",
    "#             preds.append(pred)\n",
    "\n",
    "#             label = np.argmax(pred, axis=1)[0]\n",
    "#             df.loc[i+idx, 'pred'] = int(label)\n",
    "# #             print(df[df['name'] == name])\n",
    "#     idx += len(to_test)\n",
    "\n",
    "#     print(\"\\nAnalyzing {}/{}\".format(counter, total))\n",
    "#     print(df[df['name'] == name])            \n",
    "#     counter += 1\n",
    "\n",
    "#         box = (im['x'], im['y'], im['w'], im['h'])\n",
    "#         rect = patches.Rectangle(\n",
    "#             (box[0], box[1]), \n",
    "#             box[2]-box[0], \n",
    "#             box[3]-box[1],\n",
    "#             linewidth=2,\n",
    "#             edgecolor=color_dict[label],\n",
    "#             facecolor='none'\n",
    "#         )\n",
    "#         ax.add_patch(rect)\n",
    "#         ax.label = \"Detecting Face Masks\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-15T20:48:31.026Z"
    }
   },
   "outputs": [],
   "source": [
    "# df[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T18:31:39.456963Z",
     "start_time": "2020-11-14T18:31:34.189Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Our Model With Video Feed\n",
    "\n",
    "+ We have two available facial detection algorithms: \n",
    "  1. Haar Cascade - faster detection, but only tuned for full, frontal views of faces.\n",
    "  2. MTCNN - slower detection, but returns extra features and can detect faces from a variety of angles -- not only the front.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-15T20:48:31.038Z"
    }
   },
   "outputs": [],
   "source": [
    "# use haar cascade to detect faces (frontal)\n",
    "# HC_FRONTAL_FACE = os.path.join(HC_DATA_DIR, \n",
    "#                                'haarcascade_frontalface_default.xml')\n",
    "# classifier = cv2.CascadeClassifier(HC_FRONTAL_FACE)\n",
    "\n",
    "\n",
    "# use MTCNN to detect faces\n",
    "classifier = MTCNN()\n",
    "\n",
    "\n",
    "labels_dict={ 0:'No Mask', 1:'Mask' }\n",
    "color_dict={ 0: (0,0,255), 1: (0,255,0) }\n",
    "\n",
    "size = 4\n",
    "\n",
    "# open webcam feed\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# alternatively, open feed from a file\n",
    "cap = cv2.VideoCapture('/Users/brtonnies/ArtificialIntelligence/memask4.mov')\n",
    "\n",
    "# EVEN from a mobile device (UNTESTED)\n",
    "# cap = cv.VideoCapture(1)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if frame is not None:\n",
    "        # frame = cv2.flip(frame, 1, 1) # flip so view looks more natural live (?)\n",
    "        \n",
    "\n",
    "        # detection is a little faster on smaller images -- so let's make it small\n",
    "        small = cv2.resize(frame, (frame.shape[1] // size, frame.shape[0] // size))\n",
    "\n",
    "        # detect faces with haar cascades\n",
    "        # faces = classifier.detectMultiScale(small)\n",
    "        \n",
    "        # detect faces with MTCNN\n",
    "        faces = classifier.detect_faces(small)\n",
    "\n",
    "        for f in faces:\n",
    "            # reverse the scale-down of the frame for bounding box (haar)\n",
    "#             (x, y, w, h) = [v * size for vstack in f] \n",
    "#             keypoints = None\n",
    "            \n",
    "            # reverse the scale-down of the frame for bounding box (MTCNN)\n",
    "            (x, y, w, h) = [v * size for v in f['box']]\n",
    "            # print(f['keypoints'])\n",
    "            keypoints = f['keypoints']\n",
    "                \n",
    "        \n",
    "            face = frame[y:y+h, x:x+w]\n",
    "            resized = cv2.resize(face, (150,150))\n",
    "            normalized = resized/255.0\n",
    "            reshaped = np.reshape(normalized,(1,150,150,3))\n",
    "            reshaped = np.vstack([reshaped])\n",
    "            result = model.predict(reshaped)\n",
    "            print(result)\n",
    "\n",
    "            label = np.argmax(result, axis=1)[0]\n",
    "#             print(\"PREDICTION: {}\".format(labels_dict[label]))\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), color_dict[label], 2)\n",
    "            cv2.rectangle(frame, (x,y-40), (x+w,y), color_dict[label], -1)\n",
    "            cv2.putText(\n",
    "                frame, \n",
    "                labels_dict[label], \n",
    "                (x, y-10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.8,\n",
    "                (255,255,255),\n",
    "                2\n",
    "            )\n",
    "            \n",
    "            # extra fun when using MTCNN :)\n",
    "            if keypoints is not None:\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['left_eye']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['right_eye']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['nose']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['mouth_left']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['mouth_right']])), \n",
    "                    2, (255,0,255), 2)\n",
    "            # end of extra fun with MTCNN\n",
    "\n",
    "        cv2.imshow('LIVE DETECTION ACTIVE', frame)\n",
    "        key = cv2.waitKey(10)\n",
    "\n",
    "        if key == 27: # if Esc key pressed -- end feed\n",
    "            break\n",
    "        \n",
    "#         clear_output(wait=True)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "# stop video\n",
    "cap.release()\n",
    "\n",
    "# close windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-15T20:48:31.040Z"
    }
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "563.736px",
    "left": "1135.07px",
    "right": "20px",
    "top": "77.9858px",
    "width": "442.983px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
