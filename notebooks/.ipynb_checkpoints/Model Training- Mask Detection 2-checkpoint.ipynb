{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T03:15:20.914146Z",
     "start_time": "2020-11-17T03:15:15.591556Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# our nice list of imports is in here\n",
    "#i hate this, but the notebook won't recognize our custom modules w/o it\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import lib\n",
    "from lib import utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.models import Sequential        # keras is only compatible with python 3.6 or lower\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "\n",
    "from skimage import io, data\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.transform import rescale\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Mask Detection\n",
    "\n",
    "\n",
    "### Brief Overview\n",
    "\n",
    "Simply put, our goal is to create a neural network that will (1) identify face masks or lack thereof on a human face, and (2) determine whether that facemask is correctly worn.\n",
    "\n",
    "\n",
    "Luckily for us, Kaggle had a contest earlier this year related to (1) -- so a dataset was readily available, and it has proven to be quite effective at training a mask/no-mask neural net with our testing images. (Kaggle)\n",
    "\n",
    "\n",
    "However, the dataset is extremely lacking when it comes to incorrect mask usage.  We did locate an additional dataset that consists only of faces with correctly worn masks and incorrectly worn masks. (CMFD/IMFD)\n",
    "\n",
    "### Data\n",
    "\n",
    "1. Training Data - We have training data from both datasets.  The images in each set have been pre-processed with relevant data written to CSV files to ease the process load.\n",
    "  + Kaggle Dataset -- \n",
    "2. Testing Data - Our testing data consists of \n",
    "\n",
    "\n",
    "### Models\n",
    "\n",
    "To that end, we need a few different models:\n",
    "1. One to predict whether there's a mask on a face or no mask\n",
    "  + For this, we will use the Kaggle Dataset, and focus on the two largest features available in it: whether there is a face mask or not.\n",
    "  + Potentially, we could add the data from the CMFD/IMFD dataset as representations of masks for the training set, since we'll be determining whether they're correct or incorrectly worn in the next stage.  The only concern there is a dataset that's very, very heavy on masks vs. no masks\n",
    "2. One to predict whether, given a mask is on a face, it is worn correctly\n",
    "  + For this, we will use the CMFD/IMFD dataset.  There are only two features to this dataset: correctly worn and incorrectly worn face masks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T03:15:20.964128Z",
     "start_time": "2020-11-17T03:15:20.925399Z"
    }
   },
   "outputs": [],
   "source": [
    "# define directory paths for easier navigation\n",
    "NOTEBOOK_DIR = !pwd\n",
    "NOTEBOOK_DIR = NOTEBOOK_DIR[0]\n",
    "ROOT_DIR = os.path.abspath(os.path.join(NOTEBOOK_DIR, os.pardir))\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'images')\n",
    "IMG_DATA_DIR = os.path.join(DATA_DIR, 'img_data')\n",
    "KAGGLE_DIR = os.path.join(IMAGES_DIR, 'kaggle')\n",
    "HC_DATA_DIR = os.path.join(DATA_DIR, 'haar_cascades')\n",
    "\n",
    "# MNM_TRIAL_DIR = os.path.join(IMAGES_DIR, 'mnm_trial')\n",
    "# MNM_TRAINING = os.path.join(MNM_TRIAL_DIR, 'training')\n",
    "# MNM_TESTING = os.path.join(MNM_TRIAL_DIR, 'testing')\n",
    "\n",
    "PRAJNA_SRC_DIR = os.path.join(DATA_DIR, 'prajna-dataset')\n",
    "PMASK_SRC = os.path.join(PRAJNA_SRC_DIR, 'mask')\n",
    "PNOMASK_SRC = os.path.join(PRAJNA_SRC_DIR, 'nomask')\n",
    "\n",
    "PTRAINING_DIR = os.path.join(PRAJNA_SRC_DIR, 'dest/train')\n",
    "PTESTING_DIR = os.path.join(PRAJNA_SRC_DIR, 'dest/test')\n",
    "PVAL_DIR = os.path.join(PRAJNA_SRC_DIR, 'dest/val')\n",
    "\n",
    "PTRAINING_MASK = os.path.join(PRAJNA_SRC_DIR, 'dest/train/mask')\n",
    "PTESTING_MASK = os.path.join(PRAJNA_SRC_DIR, 'dest/test/mask')\n",
    "PVAL_MASK = os.path.join(PRAJNA_SRC_DIR, 'dest/val/mask')\n",
    "\n",
    "PTRAINING_NOMASK = os.path.join(PRAJNA_SRC_DIR, 'dest/train/nomask')\n",
    "PTESTING_NOMASK = os.path.join(PRAJNA_SRC_DIR, 'dest/test/nomask')\n",
    "PVAL_NOMASK = os.path.join(PRAJNA_SRC_DIR, 'dest/val/nomask')\n",
    "\n",
    "\n",
    "EXTENDED_SRC_DIR = os.path.join(DATA_DIR, 'extended-dataset')\n",
    "EMASK_SRC = os.path.join(EXTENDED_SRC_DIR, 'mask')\n",
    "ENOMASK_SRC = os.path.join(EXTENDED_SRC_DIR, 'nomask')\n",
    "\n",
    "ETRAINING_DIR = os.path.join(EXTENDED_SRC_DIR, 'dest/train')\n",
    "ETESTING_DIR = os.path.join(EXTENDED_SRC_DIR, 'dest/test')\n",
    "EVAL_DIR = os.path.join(EXTENDED_SRC_DIR, 'dest/val')\n",
    "\n",
    "ETRAINING_MASK = os.path.join(EXTENDED_SRC_DIR, 'dest/train/mask')\n",
    "ETESTING_MASK = os.path.join(EXTENDED_SRC_DIR, 'dest/test/mask')\n",
    "EVAL_MASK = os.path.join(EXTENDED_SRC_DIR, 'dest/val/mask')\n",
    "\n",
    "ETRAINING_NOMASK = os.path.join(EXTENDED_SRC_DIR, 'dest/train/nomask')\n",
    "ETESTING_NOMASK = os.path.join(EXTENDED_SRC_DIR, 'dest/test/nomask')\n",
    "EVAL_NOMASK = os.path.join(EXTENDED_SRC_DIR, 'dest/val/nomask')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MASK_SRC_DIR=os.path.join(MNM_TRIAL_DIR, 'mask')\n",
    "# TRAINING_MASK=os.path.join(MNM_TRAINING, 'mask')\n",
    "# TESTING_MASK=os.path.join(MNM_TESTING, 'mask')\n",
    "\n",
    "# NOMASK_SRC_DIR=os.path.join(MNM_TRIAL_DIR, 'nomask')\n",
    "# TRAINING_NOMASK=os.path.join(MNM_TRAINING, 'nomask')\n",
    "# TESTING_NOMASK=os.path.join(MNM_TESTING, 'nomask')\n",
    "\n",
    "KAGGLE_IMAGES_DIR = \"https://github.com/brtonnies/face-mask-detection/blob/data-branch/data/images\"\n",
    "# CMFD_IMAGES_DIR = \"https://github.com/brtonnies/face-mask-detection/blob/data-branch/data/images/CMFD/images\"\n",
    "# IMFD_IMAGES_DIR = \"https://github.com/brtonnies/face-mask-detection/blob/data-branch/data/images/IMFD/images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Mask vs. No Mask (MNM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:06.319969Z",
     "start_time": "2020-11-12T20:23:06.317684Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:06.660624Z",
     "start_time": "2020-11-12T20:23:06.658847Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparations\n",
    "\n",
    "+ We keep pre-compiled versions of our different models on hand (again, to reduce process/load times)\n",
    "+ Unless the training data changes, we default to using those existing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:08.127649Z",
     "start_time": "2020-11-12T20:23:08.125432Z"
    }
   },
   "source": [
    "1. First model used ONLY Prajna Dataset (97% acc)\n",
    "2. Second model is training now, with additional 'no mask' images from Kaggle\n",
    "3. Third will have additional 'mask' images from kaggle\n",
    "\n",
    "**DON'T FORGET TO REVERT IMAGES TO BEST MODEL VERSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:08.590623Z",
     "start_time": "2020-11-12T20:23:08.588383Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T03:15:20.979633Z",
     "start_time": "2020-11-17T03:15:20.977507Z"
    }
   },
   "outputs": [],
   "source": [
    "# def data_summary(main_path):\n",
    "    \n",
    "#     yes_path = main_path+'/mask'\n",
    "#     no_path = main_path+'/nomask'\n",
    "        \n",
    "#     # number of files (images) that are in the the folder named 'yes' that represent tumorous (positive) examples\n",
    "#     m_pos = len(os.listdir(yes_path))\n",
    "#     # number of files (images) that are in the the folder named 'no' that represent non-tumorous (negative) examples\n",
    "#     m_neg = len(os.listdir(no_path))\n",
    "#     # number of all examples\n",
    "#     m = (m_pos+m_neg)\n",
    "    \n",
    "#     pos_prec = (m_pos* 100.0)/ m\n",
    "#     neg_prec = (m_neg* 100.0)/ m\n",
    "    \n",
    "#     print(f\"Number of examples: {m}\")\n",
    "#     print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\") \n",
    "#     print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\") \n",
    "    \n",
    "# augmented_data_path = os.path.join(EXTENDED_SRC_DIR, 'dest')\n",
    "# data_summary(augmented_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T03:15:26.966773Z",
     "start_time": "2020-11-17T03:15:22.679272Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(src, train, test, split_size):\n",
    "    dataset = []\n",
    "    \n",
    "    for unitData in os.listdir(src):\n",
    "        data = os.path.join(src, unitData)\n",
    "        if(os.path.getsize(data) > 0 and os.path.isfile(data)):\n",
    "            dataset.append(unitData)\n",
    "        else:\n",
    "            print('Skipped ' + unitData)\n",
    "            print('Invalid file i.e zero size')\n",
    "    \n",
    "    train_set_length = int(len(dataset) * split_size)\n",
    "    test_set_length = int(len(dataset) - train_set_length)\n",
    "    shuffled_set = random.sample(dataset, len(dataset))\n",
    "    train_set = dataset[0:train_set_length]\n",
    "    test_set = dataset[-test_set_length:]\n",
    "       \n",
    "    for unitData in train_set:\n",
    "        temp_train_set = os.path.join(src, unitData)\n",
    "        final_train_set = os.path.join(train, unitData)\n",
    "        \n",
    "        copyfile(temp_train_set, final_train_set)\n",
    "    \n",
    "    for unitData in test_set:\n",
    "        temp_test_set = os.path.join(src, unitData)\n",
    "        final_test_set = os.path.join(test, unitData)\n",
    "        copyfile(temp_test_set, final_test_set)\n",
    "\n",
    "        \n",
    "split_size = 0.8\n",
    "split_data(EMASK_SRC, ETRAINING_MASK, ETESTING_MASK, split_size)\n",
    "split_data(ENOMASK_SRC, ETRAINING_NOMASK, ETESTING_NOMASK, split_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:11.386097Z",
     "start_time": "2020-11-12T20:23:11.384123Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:11.807979Z",
     "start_time": "2020-11-12T20:23:11.805661Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-11T19:57:01.408Z"
    }
   },
   "source": [
    "## Keras Sequential Model\n",
    "\n",
    "#### Building the Model\n",
    "In this proposed method, the Face Mask detection model is built using the Sequential API of the keras library. This allows us to create the new layers for our model step by step. The various layers used for our CNN model is described below.\n",
    "\n",
    "1. The first layer is Conv2D with **100 kernels** of size $3\\times3$. \n",
    "  + The activation function is ‘ReLu’.  \n",
    "  + The input size is set to $150\\times 150 \\times 3$ for all the images to be trained and tested\n",
    "\n",
    "2. MaxPooling2D is used for the second layer with pool size of $2 \\times 2$.\n",
    "\n",
    "3. The next layer is another Conv2D layer \n",
    "  + Has its own 100 kernels of the same size: $3\\times3$ \n",
    "  + Activation function is ‘ReLu’. \n",
    "  + The layer is followed by another MaxPooling2D layer with pool size $2 \\times 2$.\n",
    "\n",
    "4. Then the Flatten() layer to flatten *all* the layers into a single dimension.\n",
    "\n",
    "5. **NEW:** After the Flatten layer, we added dropout (0.5) layer to prevent the model from overfitting -- which was killing us in early trials.\n",
    "\n",
    "6. Then we used the Dense layer \n",
    "  + 50 'units' \n",
    "  + Activation function: ‘ReLu’.\n",
    "\n",
    "7. The last layer of our model will be another Dense Layer, with only two units and activation function ‘Softmax’. The softmax function outputs a vector representing the probability distributions of each of the input units. Here, two input units are used. The softmax function will output a vector with two probability distribution values: is there a mask on this face or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T03:15:30.361193Z",
     "start_time": "2020-11-17T03:15:30.180772Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "kernel_size = (3, 3)\n",
    "image_shape = (150, 150, 3)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(100, kernel_size, activation='tanh', input_shape=image_shape, strides=2),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "#     tf.keras.layers.Conv2D(100, (3,3), activation='tanh'),\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(100, kernel_size, activation='tanh'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.2),   # previously we were way overfitting, so we added dropout\n",
    "    tf.keras.layers.Dense(50, activation='tanh'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax'),\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-4, epsilon=1e-2)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Training/Testing/Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T03:15:33.679008Z",
     "start_time": "2020-11-17T03:15:32.887950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5040 images belonging to 2 classes.\n",
      "Found 1301 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(ETRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(150, 150))\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(ETESTING_DIR, \n",
    "                                                              batch_size=10, \n",
    "                                                              target_size=(150, 150))\n",
    "\n",
    "# (1) train the all data extended model\n",
    "# (2) re-train prajna, prajna-kaggle-mask to save new checkpoints (and load them :D)\n",
    "# (3) also try and do one model where only new masks are added to the set\n",
    "\n",
    "# model_path = '../models/mnm/prajna'\n",
    "# checkpoint_path = '../models/mnm/prajna.checkpoint' \n",
    "\n",
    "model_path = '../models/mnm/extended_dataset_tanh_3e41e2'\n",
    "checkpoint_path = '../models/mnm/extended_datasest_tanh_3e41e2.checkpoint'\n",
    "\n",
    "# model_path = '../models/mnm/prajna-kaggle-nomask-extend'\n",
    "# checkpoint_path = '../models/mnm/prajna-kaggle-nomask-extend-{epoch:03d}.model'\n",
    "# model_path = '../models/mnm/prajna-kaggle-mask-extend'\n",
    "# checkpoint_path = '../models/mnm/prajna-kaggle-mask-extend-{epoch:03d}.model'\n",
    "# model_path = '../models/mnm/prajna-kaggle-all-extend'\n",
    "# checkpoint_path = '../models/mnm/prajna-kaggle-all-extend-{epoch:03d}.model'\n",
    "\n",
    "\n",
    "# NOTE: here's something cool - we can use checkpoints\n",
    "#       to save/load the weights from previous model training\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     model.load_weights(os.path.join(checkpoint_path, 'variables/variables'))\n",
    "    \n",
    "# alternatively, we can load the whole model from the checkpoint state\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model = load_model(checkpoint_path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path, \n",
    "    monitor = 'val_loss', \n",
    "    verbose = 1, \n",
    "    save_best_only = True,\n",
    "    mode = 'auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T03:59:40.773983Z",
     "start_time": "2020-11-17T03:15:43.595619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "503/504 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8416\n",
      "Epoch 00001: val_loss improved from inf to 0.28877, saving model to ../models/mnm/extended_datasest_tanh_3e41e2.checkpoint\n",
      "WARNING:tensorflow:From /Users/brtonnies/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/extended_datasest_tanh_3e41e2.checkpoint/assets\n",
      "504/504 [==============================] - 291s 578ms/step - loss: 0.3653 - acc: 0.8413 - val_loss: 0.2888 - val_acc: 0.8855\n",
      "Epoch 2/30\n",
      "503/504 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.8469\n",
      "Epoch 00002: val_loss did not improve from 0.28877\n",
      "504/504 [==============================] - 291s 577ms/step - loss: 0.3544 - acc: 0.8468 - val_loss: 0.3139 - val_acc: 0.8647\n",
      "Epoch 3/30\n",
      "503/504 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8521\n",
      "Epoch 00003: val_loss did not improve from 0.28877\n",
      "504/504 [==============================] - 308s 611ms/step - loss: 0.3451 - acc: 0.8518 - val_loss: 0.3203 - val_acc: 0.8716\n",
      "Epoch 4/30\n",
      "503/504 [============================>.] - ETA: 0s - loss: 0.3423 - acc: 0.8549\n",
      "Epoch 00004: val_loss improved from 0.28877 to 0.28111, saving model to ../models/mnm/extended_datasest_tanh_3e41e2.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/extended_datasest_tanh_3e41e2.checkpoint/assets\n",
      "504/504 [==============================] - 314s 624ms/step - loss: 0.3425 - acc: 0.8550 - val_loss: 0.2811 - val_acc: 0.8909\n",
      "Epoch 5/30\n",
      "503/504 [============================>.] - ETA: 0s - loss: 0.3557 - acc: 0.8525\n",
      "Epoch 00005: val_loss improved from 0.28111 to 0.27712, saving model to ../models/mnm/extended_datasest_tanh_3e41e2.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/extended_datasest_tanh_3e41e2.checkpoint/assets\n",
      "504/504 [==============================] - 307s 609ms/step - loss: 0.3555 - acc: 0.8524 - val_loss: 0.2771 - val_acc: 0.9008\n",
      "Epoch 6/30\n",
      "503/504 [============================>.] - ETA: 0s - loss: 0.3480 - acc: 0.8507\n",
      "Epoch 00006: val_loss improved from 0.27712 to 0.26436, saving model to ../models/mnm/extended_datasest_tanh_3e41e2.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/extended_datasest_tanh_3e41e2.checkpoint/assets\n",
      "504/504 [==============================] - 288s 571ms/step - loss: 0.3481 - acc: 0.8506 - val_loss: 0.2644 - val_acc: 0.8993\n",
      "Epoch 7/30\n",
      "503/504 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8515\n",
      "Epoch 00007: val_loss did not improve from 0.26436\n",
      "504/504 [==============================] - 353s 701ms/step - loss: 0.3499 - acc: 0.8516 - val_loss: 0.2716 - val_acc: 0.9039\n",
      "Epoch 8/30\n",
      "503/504 [============================>.] - ETA: 0s - loss: 0.3384 - acc: 0.8557\n",
      "Epoch 00008: val_loss did not improve from 0.26436\n",
      "504/504 [==============================] - 389s 773ms/step - loss: 0.3383 - acc: 0.8558 - val_loss: 0.2659 - val_acc: 0.8885\n",
      "Epoch 9/30\n",
      "143/504 [=======>......................] - ETA: 3:58 - loss: 0.3260 - acc: 0.8559"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fb50dbaa623d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                               callbacks=[checkpoint])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    706\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\u001b[0m in \u001b[0;36mreturn_outputs_and_add_losses\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs_arg_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs_arg_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=30,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Did We Do?\n",
    "\n",
    "To answer that, we have to answer a few other questions:\n",
    "1. What do the training/validation results look like?\n",
    "  + Can we accurately predict the data in the dataset source images?\n",
    "2. How well does our model predict a completely different dataset?\n",
    "  + For this, we'll use the training set from Kaggle so we can also check our accuracy\n",
    "3. How well does our model predict in a different setting?\n",
    "  + We'll use a live WebCam feed or video file and have our model predict in real time (or close to it -- framerate is spotty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T03:15:38.684118Z",
     "start_time": "2020-11-17T03:15:38.264200Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ac3b8e725912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mplot_accuracy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_accuracy_loss(history):\n",
    "    \"\"\"\n",
    "        Plot the accuracy and the loss during the training of the nn.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(18,12))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(221)\n",
    "    plt.plot(history.history['acc'],'bo--', label = \"acc\")\n",
    "    plt.plot(history.history['val_acc'], 'ro--', label = \"val_acc\")\n",
    "    plt.title(\"train_acc vs val_acc\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss function\n",
    "    plt.subplot(222)\n",
    "    plt.plot(history.history['loss'],'bo--', label = \"loss\")\n",
    "    plt.plot(history.history['val_loss'], 'ro--', label = \"val_loss\")\n",
    "    plt.title(\"train_loss vs val_loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_accuracy_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting The Current Dataset\n",
    "\n",
    "+ We'll start with the images in the source directory of masks\n",
    "  1. First we go through the images, use facial detection to find bounding boxes and/or keypoints\n",
    "  2. Then we  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T20:14:37.169683Z",
     "start_time": "2020-11-15T20:14:37.156639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(PMASK_SRC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Another Dataset\n",
    "\n",
    "Our model's performance in the training/validation stage was admirable.  Let's give it some images from another dataset and see how it performs with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T20:14:37.176145Z",
     "start_time": "2020-11-15T20:14:37.173506Z"
    }
   },
   "outputs": [],
   "source": [
    "# we'll be using the Kaggle mask detection contest training dataset\n",
    "# df = pd.read_csv(os.path.join(IMG_DATA_DIR, 'kaggle_training.csv'), index_col=0)\n",
    "\n",
    "# # the kaggle dataset actually had errors in its labeling -- on purpose\n",
    "# df.rename(columns={'x1': 'x', 'x2': 'y', 'y1': 'w', 'y2': 'h'}, inplace=True)\n",
    "# df.drop(['bbox'], axis=1, inplace=True)\n",
    "\n",
    "# df = df[df['classname'].isin(['face_with_mask', 'face_with_mask_incorrect', 'face_no_mask'])]\n",
    "\n",
    "\n",
    "# # df['pred'] = np.nan\n",
    "                      \n",
    "# test_images = list(df['name'].unique())\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T20:14:37.197688Z",
     "start_time": "2020-11-15T20:14:37.179901Z"
    }
   },
   "outputs": [],
   "source": [
    "# labels_dict={ 0:'No Mask', 1:'Mask' }\n",
    "# color_dict={ 0: 'magenta', 1: 'cyan' }\n",
    "\n",
    "# # name = test_images[0]\n",
    "# total = len(test_images)\n",
    "# counter = 1\n",
    "# idx = 0\n",
    "# for name in test_images:\n",
    "#     # fetch the image from the remote repository\n",
    "#     url = os.path.join(KAGGLE_IMAGES_DIR, \"{}?raw=true\".format(name))\n",
    "#     resp = urllib.request.urlopen(url)\n",
    "#     image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "#     image = cv2.imdecode(image, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "#     # locate the image within the dataframe so we can find bounding boxes\n",
    "#     # NOTE: bounding boxes were pre-detected in this dataset and stored in the dataframe CSV\n",
    "#     to_test = df[df['name'] == name]\n",
    "#     #     print(to_test)\n",
    "\n",
    "\n",
    "# #     fig, ax = plt.subplots(1)\n",
    "# #     ax.imshow(image, cmap='gray')\n",
    "\n",
    "#     preds = list()\n",
    "#     # print([i for i in list(faces) if len(i) > 4])\n",
    "#     for i in range(len(to_test)):\n",
    "#         print(\"Index: {}\".format(i+idx))\n",
    "#         im = df.iloc[i+idx]\n",
    "\n",
    "#         face_region = image[im['y']:im['y']+im['h'], im['x']:im['x']+im['w']]\n",
    "#         if face_region.size > 0:\n",
    "#             resized = cv2.resize(face_region, (450, 450))\n",
    "#             normalized = resized/255.0\n",
    "#             reshaped = np.reshape(normalized, (-1, 150, 150, 3))\n",
    "#             reshaped = np.vstack([reshaped])\n",
    "#             pred = model.predict(reshaped)\n",
    "#             preds.append(pred)\n",
    "\n",
    "#             label = np.argmax(pred, axis=1)[0]\n",
    "#             df.loc[i+idx, 'pred'] = int(label)\n",
    "# #             print(df[df['name'] == name])\n",
    "#     idx += len(to_test)\n",
    "\n",
    "#     print(\"\\nAnalyzing {}/{}\".format(counter, total))\n",
    "#     print(df[df['name'] == name])            \n",
    "#     counter += 1\n",
    "\n",
    "#         box = (im['x'], im['y'], im['w'], im['h'])\n",
    "#         rect = patches.Rectangle(\n",
    "#             (box[0], box[1]), \n",
    "#             box[2]-box[0], \n",
    "#             box[3]-box[1],\n",
    "#             linewidth=2,\n",
    "#             edgecolor=color_dict[label],\n",
    "#             facecolor='none'\n",
    "#         )\n",
    "#         ax.add_patch(rect)\n",
    "#         ax.label = \"Detecting Face Masks\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T20:14:37.212911Z",
     "start_time": "2020-11-15T20:14:37.207541Z"
    }
   },
   "outputs": [],
   "source": [
    "# df[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T18:31:39.456963Z",
     "start_time": "2020-11-14T18:31:34.189Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Our Model With Video Feed\n",
    "\n",
    "+ We have two available facial detection algorithms: \n",
    "  1. Haar Cascade - faster detection, but only tuned for full, frontal views of faces.\n",
    "  2. MTCNN - slower detection, but returns extra features and can detect faces from a variety of angles -- not only the front.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T20:17:44.876863Z",
     "start_time": "2020-11-15T20:16:13.004332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01532199 0.98467803]]\n",
      "[[0.01435884 0.9856412 ]]\n",
      "[[0.0143001  0.98569995]]\n",
      "[[0.01481791 0.9851821 ]]\n",
      "[[0.0135907 0.9864093]]\n",
      "[[0.01364112 0.9863588 ]]\n",
      "[[0.01368821 0.98631173]]\n",
      "[[0.01432569 0.9856743 ]]\n",
      "[[0.01382948 0.98617053]]\n",
      "[[0.01413342 0.9858666 ]]\n",
      "[[0.01441228 0.9855877 ]]\n",
      "[[0.01405014 0.9859498 ]]\n",
      "[[0.01432328 0.98567677]]\n",
      "[[0.01486297 0.985137  ]]\n",
      "[[0.0146727  0.98532736]]\n",
      "[[0.01419461 0.98580533]]\n",
      "[[0.01439117 0.98560876]]\n",
      "[[0.01485585 0.9851442 ]]\n",
      "[[0.01318072 0.98681927]]\n",
      "[[0.01518319 0.98481673]]\n",
      "[[0.01327248 0.98672754]]\n",
      "[[0.01390162 0.9860984 ]]\n",
      "[[0.01481052 0.9851895 ]]\n",
      "[[0.01413822 0.9858618 ]]\n",
      "[[0.01395603 0.98604393]]\n",
      "[[0.0148591 0.9851409]]\n",
      "[[0.01428584 0.9857142 ]]\n",
      "[[0.01359242 0.9864075 ]]\n",
      "[[0.01428841 0.98571163]]\n",
      "[[0.01443548 0.98556453]]\n",
      "[[0.01427999 0.98572   ]]\n",
      "[[0.01397451 0.9860255 ]]\n",
      "[[0.01570396 0.9842961 ]]\n",
      "[[0.01478496 0.985215  ]]\n",
      "[[0.01411957 0.98588043]]\n",
      "[[0.01484281 0.98515725]]\n",
      "[[0.01447416 0.9855259 ]]\n",
      "[[0.01382437 0.9861756 ]]\n",
      "[[0.01433836 0.98566157]]\n",
      "[[0.01447024 0.98552984]]\n",
      "[[0.01436903 0.98563105]]\n",
      "[[0.01415585 0.98584414]]\n",
      "[[0.01319205 0.986808  ]]\n",
      "[[0.01392439 0.9860756 ]]\n",
      "[[0.01390657 0.9860934 ]]\n",
      "[[0.01400971 0.9859903 ]]\n",
      "[[0.01458569 0.98541427]]\n",
      "[[0.01166812 0.98833185]]\n",
      "[[0.01376692 0.9862331 ]]\n",
      "[[0.01435281 0.98564726]]\n",
      "[[0.01458271 0.9854173 ]]\n",
      "[[0.0131252 0.9868749]]\n",
      "[[0.01136844 0.98863155]]\n",
      "[[0.01023632 0.9897636 ]]\n",
      "[[0.01408087 0.9859191 ]]\n",
      "[[0.01100747 0.9889925 ]]\n",
      "[[0.01392748 0.98607254]]\n",
      "[[0.01393491 0.98606515]]\n",
      "[[0.0104858  0.98951423]]\n",
      "[[0.01338389 0.98661613]]\n",
      "[[0.01479346 0.98520654]]\n",
      "[[0.01476639 0.9852336 ]]\n",
      "[[0.01476384 0.98523617]]\n",
      "[[0.01370445 0.9862955 ]]\n",
      "[[0.01504298 0.98495704]]\n",
      "[[0.01473146 0.98526853]]\n",
      "[[0.01143755 0.98856246]]\n",
      "[[0.01151419 0.9884858 ]]\n",
      "[[0.01330511 0.98669493]]\n",
      "[[0.01364542 0.98635453]]\n",
      "[[0.01370959 0.9862904 ]]\n",
      "[[0.01451193 0.985488  ]]\n",
      "[[0.01454929 0.98545074]]\n",
      "[[0.01453223 0.98546773]]\n",
      "[[0.0119849 0.9880151]]\n",
      "[[0.01408584 0.9859141 ]]\n",
      "[[0.01431583 0.98568416]]\n",
      "[[0.01425547 0.98574454]]\n",
      "[[0.01430009 0.98569995]]\n",
      "[[0.01380612 0.98619395]]\n",
      "[[0.01384571 0.98615426]]\n",
      "[[0.01400489 0.9859951 ]]\n",
      "[[0.01373916 0.98626083]]\n",
      "[[0.01512358 0.9848764 ]]\n",
      "[[0.01187325 0.98812675]]\n",
      "[[0.01387922 0.98612076]]\n",
      "[[0.01402446 0.98597556]]\n",
      "[[0.01498098 0.985019  ]]\n",
      "[[0.01383103 0.986169  ]]\n",
      "[[0.01602568 0.9839743 ]]\n",
      "[[0.01442592 0.98557407]]\n",
      "[[0.01381972 0.98618025]]\n",
      "[[0.01381492 0.98618513]]\n",
      "[[0.01450017 0.9854998 ]]\n",
      "[[0.01464537 0.98535466]]\n",
      "[[0.01452998 0.98547006]]\n",
      "[[0.01509779 0.9849022 ]]\n",
      "[[0.01360041 0.98639953]]\n",
      "[[0.01445518 0.98554486]]\n",
      "[[0.01577485 0.98422515]]\n",
      "[[0.01354918 0.9864508 ]]\n",
      "[[0.01140398 0.988596  ]]\n",
      "[[0.01409979 0.9859002 ]]\n",
      "[[0.01414692 0.9858531 ]]\n",
      "[[0.0146454  0.98535454]]\n",
      "[[0.01457478 0.9854253 ]]\n",
      "[[0.01345952 0.9865405 ]]\n",
      "[[0.01362447 0.9863755 ]]\n",
      "[[0.01389292 0.9861071 ]]\n",
      "[[0.01362454 0.9863754 ]]\n",
      "[[0.01355282 0.9864472 ]]\n",
      "[[0.01389723 0.9861028 ]]\n",
      "[[0.01376489 0.9862351 ]]\n",
      "[[0.01424989 0.9857501 ]]\n",
      "[[0.01419669 0.98580325]]\n",
      "[[0.01410255 0.9858974 ]]\n",
      "[[0.01429185 0.9857082 ]]\n",
      "[[0.01409173 0.9859082 ]]\n",
      "[[0.01450072 0.98549926]]\n",
      "[[0.01375767 0.9862423 ]]\n",
      "[[0.01407641 0.98592365]]\n",
      "[[0.01450756 0.9854924 ]]\n",
      "[[0.01481583 0.9851842 ]]\n",
      "[[0.01136966 0.9886303 ]]\n",
      "[[0.01554095 0.98445904]]\n",
      "[[0.01188974 0.9881103 ]]\n",
      "[[0.01558768 0.9844124 ]]\n",
      "[[0.01373237 0.9862677 ]]\n",
      "[[0.01504576 0.98495424]]\n",
      "[[0.0138674 0.9861326]]\n",
      "[[0.0141322 0.9858678]]\n",
      "[[0.01409941 0.9859006 ]]\n",
      "[[0.01431153 0.98568845]]\n",
      "[[0.01396092 0.98603904]]\n",
      "[[0.01474816 0.9852519 ]]\n",
      "[[0.01457169 0.9854283 ]]\n",
      "[[0.01404304 0.985957  ]]\n",
      "[[0.01376786 0.9862322 ]]\n",
      "[[0.01517872 0.98482126]]\n",
      "[[0.01307055 0.9869294 ]]\n",
      "[[0.01417896 0.985821  ]]\n",
      "[[0.01327597 0.9867241 ]]\n",
      "[[0.01418675 0.9858132 ]]\n",
      "[[0.01552068 0.98447925]]\n",
      "[[0.0144277 0.9855723]]\n",
      "[[0.0140787 0.9859213]]\n",
      "[[0.01392227 0.9860778 ]]\n",
      "[[0.01446404 0.985536  ]]\n",
      "[[0.01424456 0.9857554 ]]\n",
      "[[0.01448053 0.9855195 ]]\n",
      "[[0.01406843 0.9859315 ]]\n",
      "[[0.01362385 0.9863761 ]]\n",
      "[[0.01404165 0.9859584 ]]\n",
      "[[0.01472782 0.98527217]]\n",
      "[[0.01254067 0.9874593 ]]\n",
      "[[0.01358845 0.98641163]]\n",
      "[[0.01251163 0.9874884 ]]\n",
      "[[0.01481812 0.98518187]]\n",
      "[[0.01268496 0.98731506]]\n",
      "[[0.01292848 0.9870716 ]]\n",
      "[[0.01570019 0.9842999 ]]\n",
      "[[0.01538898 0.984611  ]]\n",
      "[[0.01444397 0.985556  ]]\n",
      "[[0.01346982 0.9865302 ]]\n",
      "[[0.01686503 0.983135  ]]\n",
      "[[0.0144726 0.9855274]]\n",
      "[[0.0151635 0.9848365]]\n",
      "[[0.01578561 0.9842144 ]]\n",
      "[[0.01600972 0.9839903 ]]\n",
      "[[0.01372007 0.98627996]]\n",
      "[[0.01363708 0.9863629 ]]\n",
      "[[0.01467834 0.9853217 ]]\n",
      "[[0.01409467 0.98590535]]\n",
      "[[0.0148148  0.98518527]]\n",
      "[[0.0157207  0.98427933]]\n",
      "[[0.01431886 0.9856812 ]]\n",
      "[[0.01642922 0.9835708 ]]\n",
      "[[0.01494148 0.98505855]]\n",
      "[[0.01439421 0.9856058 ]]\n",
      "[[0.01438326 0.9856168 ]]\n",
      "[[0.01391599 0.98608404]]\n",
      "[[0.01452393 0.9854761 ]]\n",
      "[[0.01444086 0.9855591 ]]\n",
      "[[0.02285544 0.97714454]]\n",
      "[[0.0770194 0.9229806]]\n",
      "[[0.05397195 0.9460281 ]]\n",
      "[[0.01753364 0.98246634]]\n",
      "[[0.11724645 0.88275355]]\n",
      "[[0.05799958 0.94200045]]\n",
      "[[0.03916861 0.96083146]]\n",
      "[[0.03197711 0.9680228 ]]\n",
      "[[0.02466231 0.9753377 ]]\n",
      "[[0.01769608 0.9823039 ]]\n",
      "[[0.01386099 0.986139  ]]\n",
      "[[0.01301223 0.9869877 ]]\n",
      "[[0.01215397 0.9878461 ]]\n",
      "[[0.01269489 0.9873051 ]]\n",
      "[[0.01228079 0.9877192 ]]\n",
      "[[0.01188227 0.98811775]]\n",
      "[[0.01245601 0.98754394]]\n",
      "[[0.01443728 0.9855627 ]]\n",
      "[[0.01439882 0.9856011 ]]\n",
      "[[0.01508653 0.9849134 ]]\n",
      "[[0.01130173 0.9886983 ]]\n",
      "[[0.01321453 0.9867855 ]]\n",
      "[[0.01251192 0.98748803]]\n",
      "[[0.0146709 0.9853291]]\n",
      "[[0.01314448 0.98685545]]\n",
      "[[0.01432746 0.9856726 ]]\n",
      "[[0.01419043 0.9858096 ]]\n",
      "[[0.01590381 0.98409617]]\n",
      "[[0.01539892 0.9846011 ]]\n",
      "[[0.01602006 0.98397994]]\n",
      "[[0.01467152 0.9853285 ]]\n",
      "[[0.0162467 0.9837533]]\n",
      "[[0.01477329 0.9852267 ]]\n",
      "[[0.01407254 0.98592746]]\n",
      "[[0.01458606 0.9854139 ]]\n",
      "[[0.01703825 0.9829617 ]]\n",
      "[[0.01516621 0.9848338 ]]\n",
      "[[0.01595412 0.98404586]]\n",
      "[[0.01523684 0.9847631 ]]\n",
      "[[0.0165623 0.9834378]]\n",
      "[[0.0160751  0.98392487]]\n",
      "[[0.01663517 0.98336476]]\n",
      "[[0.01508491 0.98491514]]\n",
      "[[0.01538044 0.98461956]]\n",
      "[[0.01599463 0.98400533]]\n",
      "[[0.01776734 0.9822327 ]]\n",
      "[[0.01671623 0.98328376]]\n",
      "[[0.01484468 0.9851553 ]]\n",
      "[[0.01609181 0.9839082 ]]\n",
      "[[0.018338 0.981662]]\n",
      "[[0.01538556 0.9846145 ]]\n",
      "[[0.0152237  0.98477626]]\n",
      "[[0.01623663 0.98376334]]\n",
      "[[0.0159523 0.9840477]]\n",
      "[[0.01464866 0.9853513 ]]\n",
      "[[0.01561829 0.98438174]]\n",
      "[[0.01651871 0.98348135]]\n",
      "[[0.01780551 0.9821945 ]]\n",
      "[[0.01529361 0.98470634]]\n",
      "[[0.01640883 0.98359114]]\n",
      "[[0.01712042 0.9828796 ]]\n",
      "[[0.01710791 0.98289216]]\n",
      "[[0.01435854 0.9856414 ]]\n",
      "[[0.0143861 0.9856139]]\n",
      "[[0.0136924  0.98630756]]\n",
      "[[0.01405824 0.9859417 ]]\n",
      "[[0.01380094 0.986199  ]]\n",
      "[[0.01422727 0.98577267]]\n",
      "[[0.01464513 0.9853549 ]]\n",
      "[[0.01469908 0.98530096]]\n",
      "[[0.01424114 0.9857589 ]]\n",
      "[[0.0134589 0.9865411]]\n",
      "[[0.01253858 0.9874614 ]]\n",
      "[[0.01358939 0.98641056]]\n",
      "[[0.01325952 0.9867404 ]]\n",
      "[[0.01483634 0.9851636 ]]\n",
      "[[0.01614085 0.9838591 ]]\n",
      "[[0.01500281 0.98499715]]\n",
      "[[0.01350258 0.98649746]]\n",
      "[[0.01389218 0.9861078 ]]\n",
      "[[0.01381508 0.9861849 ]]\n",
      "[[0.0146341  0.98536587]]\n",
      "[[0.01594987 0.9840501 ]]\n",
      "[[0.01596473 0.98403525]]\n",
      "[[0.01603417 0.9839659 ]]\n",
      "[[0.01632649 0.98367345]]\n",
      "[[0.01451097 0.9854891 ]]\n",
      "[[0.01511972 0.9848802 ]]\n",
      "[[0.01517967 0.9848203 ]]\n",
      "[[0.01534677 0.9846532 ]]\n",
      "[[0.01392928 0.9860707 ]]\n",
      "[[0.01405969 0.98594034]]\n",
      "[[0.01506003 0.98494   ]]\n",
      "[[0.01300151 0.9869985 ]]\n",
      "[[0.01355632 0.98644376]]\n",
      "[[0.01496818 0.98503184]]\n",
      "[[0.0130232 0.9869768]]\n",
      "[[0.01283139 0.98716855]]\n",
      "[[0.01215912 0.98784083]]\n",
      "[[0.0131129  0.98688704]]\n",
      "[[0.01184435 0.9881556 ]]\n",
      "[[0.01190353 0.9880965 ]]\n",
      "[[0.01353864 0.98646134]]\n",
      "[[0.01595964 0.98404044]]\n"
     ]
    }
   ],
   "source": [
    "# use haar cascade to detect faces (frontal)\n",
    "# HC_FRONTAL_FACE = os.path.join(HC_DATA_DIR, \n",
    "#                                'haarcascade_frontalface_default.xml')\n",
    "# classifier = cv2.CascadeClassifier(HC_FRONTAL_FACE)\n",
    "\n",
    "\n",
    "# use MTCNN to detect faces\n",
    "classifier = MTCNN()\n",
    "\n",
    "\n",
    "labels_dict={ 0:'No Mask', 1:'Mask' }\n",
    "color_dict={ 0: (0,0,255), 1: (0,255,0) }\n",
    "\n",
    "size = 4\n",
    "\n",
    "# open webcam feed\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# alternatively, open feed from a file\n",
    "cap = cv2.VideoCapture('/Users/brtonnies/ArtificialIntelligence/memask4.mov')\n",
    "\n",
    "# EVEN from a mobile device (UNTESTED)\n",
    "# cap = cv.VideoCapture(1)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if frame is not None:\n",
    "        # frame = cv2.flip(frame, 1, 1) # flip so view looks more natural live (?)\n",
    "        \n",
    "\n",
    "        # detection is a little faster on smaller images -- so let's make it small\n",
    "        small = cv2.resize(frame, (frame.shape[1] // size, frame.shape[0] // size))\n",
    "\n",
    "        # detect faces with haar cascades\n",
    "        # faces = classifier.detectMultiScale(small)\n",
    "        \n",
    "        # detect faces with MTCNN\n",
    "        faces = classifier.detect_faces(small)\n",
    "\n",
    "        for f in faces:\n",
    "            # reverse the scale-down of the frame for bounding box (haar)\n",
    "#             (x, y, w, h) = [v * size for vstack in f] \n",
    "#             keypoints = None\n",
    "            \n",
    "            # reverse the scale-down of the frame for bounding box (MTCNN)\n",
    "            (x, y, w, h) = [v * size for v in f['box']]\n",
    "            # print(f['keypoints'])\n",
    "            keypoints = f['keypoints']\n",
    "                \n",
    "        \n",
    "            face = frame[y:y+h, x:x+w]\n",
    "            resized = cv2.resize(face, (150,150))\n",
    "            normalized = resized/255.0\n",
    "            reshaped = np.reshape(normalized,(1,150,150,3))\n",
    "            reshaped = np.vstack([reshaped])\n",
    "            result = model.predict(reshaped)\n",
    "            print(result)\n",
    "\n",
    "            label = np.argmax(result, axis=1)[0]\n",
    "#             print(\"PREDICTION: {}\".format(labels_dict[label]))\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), color_dict[label], 2)\n",
    "            cv2.rectangle(frame, (x,y-40), (x+w,y), color_dict[label], -1)\n",
    "            cv2.putText(\n",
    "                frame, \n",
    "                labels_dict[label], \n",
    "                (x, y-10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.8,\n",
    "                (255,255,255),\n",
    "                2\n",
    "            )\n",
    "            \n",
    "            # extra fun when using MTCNN :)\n",
    "            if keypoints is not None:\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['left_eye']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['right_eye']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['nose']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['mouth_left']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['mouth_right']])), \n",
    "                    2, (255,0,255), 2)\n",
    "            # end of extra fun with MTCNN\n",
    "\n",
    "        cv2.imshow('LIVE DETECTION ACTIVE', frame)\n",
    "        key = cv2.waitKey(10)\n",
    "\n",
    "        if key == 27: # if Esc key pressed -- end feed\n",
    "            break\n",
    "        \n",
    "#         clear_output(wait=True)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "# stop video\n",
    "cap.release()\n",
    "\n",
    "# close windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T20:16:08.748533Z",
     "start_time": "2020-11-15T20:16:08.745107Z"
    }
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "563.736px",
    "left": "1135.07px",
    "right": "20px",
    "top": "77.9858px",
    "width": "442.983px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
