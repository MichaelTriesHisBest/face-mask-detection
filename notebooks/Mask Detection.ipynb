{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:04:21.552270Z",
     "start_time": "2020-11-14T01:04:16.453213Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# our nice list of imports is in here\n",
    "#i hate this, but the notebook won't recognize our custom modules w/o it\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import lib\n",
    "from lib import utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.models import Sequential        # keras is only compatible with python 3.6 or lower\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "\n",
    "from skimage import io, data\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.transform import rescale\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Mask Detection\n",
    "\n",
    "\n",
    "### Brief Overview\n",
    "\n",
    "Simply put, our goal is to create a neural network that will (1) identify face masks or lack thereof on a human face, and (2) determine whether that facemask is correctly worn.\n",
    "\n",
    "\n",
    "Luckily for us, Kaggle had a contest earlier this year related to (1) -- so a dataset was readily available, and it has proven to be quite effective at training a mask/no-mask neural net with our testing images. (Kaggle)\n",
    "\n",
    "\n",
    "However, the dataset is extremely lacking when it comes to incorrect mask usage.  We did locate an additional dataset that consists only of faces with correctly worn masks and incorrectly worn masks. (CMFD/IMFD)\n",
    "\n",
    "### Data\n",
    "\n",
    "1. Training Data - We have training data from both datasets.  The images in each set have been pre-processed with relevant data written to CSV files to ease the process load.\n",
    "  + Kaggle Dataset -- \n",
    "2. Testing Data - Our testing data consists of \n",
    "\n",
    "\n",
    "### Models\n",
    "\n",
    "To that end, we need a few different models:\n",
    "1. One to predict whether there's a mask on a face or no mask\n",
    "  + For this, we will use the Kaggle Dataset, and focus on the two largest features available in it: whether there is a face mask or not.\n",
    "  + Potentially, we could add the data from the CMFD/IMFD dataset as representations of masks for the training set, since we'll be determining whether they're correct or incorrectly worn in the next stage.  The only concern there is a dataset that's very, very heavy on masks vs. no masks\n",
    "2. One to predict whether, given a mask is on a face, it is worn correctly\n",
    "  + For this, we will use the CMFD/IMFD dataset.  There are only two features to this dataset: correctly worn and incorrectly worn face masks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:04:21.579948Z",
     "start_time": "2020-11-14T01:04:21.554245Z"
    }
   },
   "outputs": [],
   "source": [
    "# define directory paths for easier navigation\n",
    "NOTEBOOK_DIR = !pwd\n",
    "NOTEBOOK_DIR = NOTEBOOK_DIR[0]\n",
    "ROOT_DIR = os.path.abspath(os.path.join(NOTEBOOK_DIR, os.pardir))\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'images')\n",
    "IMG_DATA_DIR = os.path.join(DATA_DIR, 'img_data')\n",
    "KAGGLE_DIR = os.path.join(IMAGES_DIR, 'kaggle')\n",
    "HC_DATA_DIR = os.path.join(DATA_DIR, 'haar_cascades')\n",
    "\n",
    "# MNM_TRIAL_DIR = os.path.join(IMAGES_DIR, 'mnm_trial')\n",
    "# MNM_TRAINING = os.path.join(MNM_TRIAL_DIR, 'training')\n",
    "# MNM_TESTING = os.path.join(MNM_TRIAL_DIR, 'testing')\n",
    "\n",
    "PRAJNA_SRC_DIR = os.path.join(DATA_DIR, 'prajna-dataset')\n",
    "PMASK_SRC = os.path.join(PRAJNA_SRC_DIR, 'mask')\n",
    "PNOMASK_SRC = os.path.join(PRAJNA_SRC_DIR, 'nomask')\n",
    "\n",
    "PTRAINING_DIR = os.path.join(PRAJNA_SRC_DIR, 'dest/train')\n",
    "PTESTING_DIR = os.path.join(PRAJNA_SRC_DIR, 'dest/test')\n",
    "PVAL_DIR = os.path.join(PRAJNA_SRC_DIR, 'dest/val')\n",
    "\n",
    "PTRAINING_MASK = os.path.join(PRAJNA_SRC_DIR, 'dest/train/mask')\n",
    "PTESTING_MASK = os.path.join(PRAJNA_SRC_DIR, 'dest/test/mask')\n",
    "PVAL_MASK = os.path.join(PRAJNA_SRC_DIR, 'dest/val/mask')\n",
    "\n",
    "PTRAINING_NOMASK = os.path.join(PRAJNA_SRC_DIR, 'dest/train/nomask')\n",
    "PTESTING_NOMASK = os.path.join(PRAJNA_SRC_DIR, 'dest/test/nomask')\n",
    "PVAL_NOMASK = os.path.join(PRAJNA_SRC_DIR, 'dest/val/nomask')\n",
    "\n",
    "\n",
    "\n",
    "# MASK_SRC_DIR=os.path.join(MNM_TRIAL_DIR, 'mask')\n",
    "# TRAINING_MASK=os.path.join(MNM_TRAINING, 'mask')\n",
    "# TESTING_MASK=os.path.join(MNM_TESTING, 'mask')\n",
    "\n",
    "# NOMASK_SRC_DIR=os.path.join(MNM_TRIAL_DIR, 'nomask')\n",
    "# TRAINING_NOMASK=os.path.join(MNM_TRAINING, 'nomask')\n",
    "# TESTING_NOMASK=os.path.join(MNM_TESTING, 'nomask')\n",
    "\n",
    "KAGGLE_IMAGES_DIR = \"https://github.com/brtonnies/face-mask-detection/blob/data-branch/data/images\"\n",
    "# CMFD_IMAGES_DIR = \"https://github.com/brtonnies/face-mask-detection/blob/data-branch/data/images/CMFD/images\"\n",
    "# IMFD_IMAGES_DIR = \"https://github.com/brtonnies/face-mask-detection/blob/data-branch/data/images/IMFD/images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Mask vs. No Mask (MNM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:06.319969Z",
     "start_time": "2020-11-12T20:23:06.317684Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:06.660624Z",
     "start_time": "2020-11-12T20:23:06.658847Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparations\n",
    "\n",
    "+ We keep pre-compiled versions of our different models on hand (again, to reduce process/load times)\n",
    "+ Unless the training data changes, we default to using those existing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:08.127649Z",
     "start_time": "2020-11-12T20:23:08.125432Z"
    }
   },
   "source": [
    "1. First model used ONLY Prajna Dataset (97% acc)\n",
    "2. Second model is training now, with additional 'no mask' images from Kaggle\n",
    "3. Third will have additional 'mask' images from kaggle\n",
    "\n",
    "**DON'T FORGET TO REVERT IMAGES TO BEST MODEL VERSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:08.590623Z",
     "start_time": "2020-11-12T20:23:08.588383Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:09.142895Z",
     "start_time": "2020-11-12T20:23:09.141052Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:04:22.103956Z",
     "start_time": "2020-11-14T01:04:21.582854Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(src, train, test, split_size):\n",
    "    dataset = []\n",
    "    \n",
    "    for unitData in os.listdir(src):\n",
    "        data = os.path.join(src, unitData)\n",
    "        if(os.path.getsize(data) > 0 and os.path.isfile(data)):\n",
    "            dataset.append(unitData)\n",
    "        else:\n",
    "            print('Skipped ' + unitData)\n",
    "            print('Invalid file i.e zero size')\n",
    "    \n",
    "    train_set_length = int(len(dataset) * split_size)\n",
    "    test_set_length = int(len(dataset) - train_set_length)\n",
    "    shuffled_set = random.sample(dataset, len(dataset))\n",
    "    train_set = dataset[0:train_set_length]\n",
    "    test_set = dataset[-test_set_length:]\n",
    "       \n",
    "    for unitData in train_set:\n",
    "        temp_train_set = os.path.join(src, unitData)\n",
    "        final_train_set = os.path.join(train, unitData)\n",
    "        \n",
    "        copyfile(temp_train_set, final_train_set)\n",
    "    \n",
    "    for unitData in test_set:\n",
    "        temp_test_set = os.path.join(src, unitData)\n",
    "        final_test_set = os.path.join(test, unitData)\n",
    "        copyfile(temp_test_set, final_test_set)\n",
    "\n",
    "        \n",
    "split_size = 0.8\n",
    "split_data(PMASK_SRC, PTRAINING_MASK, PTESTING_MASK, split_size)\n",
    "split_data(PNOMASK_SRC, PTRAINING_NOMASK, PTESTING_NOMASK, split_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:11.386097Z",
     "start_time": "2020-11-12T20:23:11.384123Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:11.807979Z",
     "start_time": "2020-11-12T20:23:11.805661Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-11T19:57:01.408Z"
    }
   },
   "source": [
    "## Keras Sequential Model\n",
    "\n",
    "#### Building the Model\n",
    "In this proposed method, the Face Mask detection model is built using the Sequential API of the keras library. This allows us to create the new layers for our model step by step. The various layers used for our CNN model is described below.\n",
    "\n",
    "1. The first layer is Conv2D with **100 kernels** of size $3\\times3$. \n",
    "  + The activation function is ‘ReLu’.  \n",
    "  + The input size is set to $150\\times 150 \\times 3$ for all the images to be trained and tested\n",
    "\n",
    "2. MaxPooling2D is used for the second layer with pool size of $2 \\times 2$.\n",
    "\n",
    "3. The next layer is another Conv2D layer \n",
    "  + Has its own 100 kernels of the same size: $3\\times3$ \n",
    "  + Activation function is ‘ReLu’. \n",
    "  + The layer is followed by another MaxPooling2D layer with pool size $2 \\times 2$.\n",
    "\n",
    "4. Then the Flatten() layer to flatten *all* the layers into a single dimension.\n",
    "\n",
    "5. **NEW:** After the Flatten layer, we added dropout (0.5) layer to prevent the model from overfitting -- which was killing us in early trials.\n",
    "\n",
    "6. Then we used the Dense layer \n",
    "  + 50 'units' \n",
    "  + Activation function: ‘ReLu’.\n",
    "\n",
    "7. The last layer of our model will be another Dense Layer, with only two units and activation function ‘Softmax’. The softmax function outputs a vector representing the probability distributions of each of the input units. Here, two input units are used. The softmax function will output a vector with two probability distribution values: is there a mask on this face or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:04:22.327145Z",
     "start_time": "2020-11-14T01:04:22.106331Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "kernel_size = (3, 3)\n",
    "image_shape = (150, 150, 3)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(100, kernel_size, activation='relu', input_shape=image_shape, strides=2),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "#     tf.keras.layers.Conv2D(32, (3,3), activation='tanh'),\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(100, kernel_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),   # previously we were way overfitting, so we added dropout\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax'),\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3, epsilon=1e-5)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:04:22.555004Z",
     "start_time": "2020-11-14T01:04:22.329596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1105 images belonging to 2 classes.\n",
      "Found 278 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(PTRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(150, 150))\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(PTESTING_DIR, \n",
    "                                                              batch_size=10, \n",
    "                                                              target_size=(150, 150))\n",
    "\n",
    "# (1) train the all data extended model\n",
    "# (2) re-train prajna, prajna-kaggle-mask to save new checkpoints (and load them :D)\n",
    "# (3) also try and do one model where only new masks are added to the set\n",
    "\n",
    "model_path = '../models/mnm/prajna'\n",
    "checkpoint_path = '../models/mnm/prajna.checkpoint' \n",
    "\n",
    "# model_path = '../models/mnm/prajna-kaggle-nomask-extend'\n",
    "# checkpoint_path = '../models/mnm/prajna-kaggle-nomask-extend-{epoch:03d}.model'\n",
    "# model_path = '../models/mnm/prajna-kaggle-mask-extend'\n",
    "# checkpoint_path = '../models/mnm/prajna-kaggle-mask-extend-{epoch:03d}.model'\n",
    "# model_path = '../models/mnm/prajna-kaggle-all-extend'\n",
    "# checkpoint_path = '../models/mnm/prajna-kaggle-all-extend-{epoch:03d}.model'\n",
    "\n",
    "\n",
    "# NOTE: here's something cool - we can use checkpoints\n",
    "#       to save/load the weights from previous model training\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     model.load_weights(os.path.join(checkpoint_path, 'variables/variables'))\n",
    "    \n",
    "# alternatively, we can load the whole model from the checkpoint state\n",
    "# if os.path.exist(checkpoint_path):\n",
    "#     model = load_model(checkpoint_path_path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path, \n",
    "    monitor = 'val_loss', \n",
    "    verbose = 1, \n",
    "    save_best_only = True,\n",
    "    mode = 'auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:32:03.807421Z",
     "start_time": "2020-11-14T01:04:22.556791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.7388\n",
      "Epoch 00001: val_loss improved from inf to 0.22289, saving model to ../models/mnm/prajna.checkpoint\n",
      "WARNING:tensorflow:From /Users/brtonnies/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/prajna.checkpoint/assets\n",
      "111/111 [==============================] - 55s 494ms/step - loss: 0.5216 - acc: 0.7376 - val_loss: 0.2229 - val_acc: 0.9173\n",
      "Epoch 2/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.8731\n",
      "Epoch 00002: val_loss improved from 0.22289 to 0.12734, saving model to ../models/mnm/prajna.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/prajna.checkpoint/assets\n",
      "111/111 [==============================] - 53s 476ms/step - loss: 0.3340 - acc: 0.8724 - val_loss: 0.1273 - val_acc: 0.9568\n",
      "Epoch 3/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.8877\n",
      "Epoch 00003: val_loss improved from 0.12734 to 0.11207, saving model to ../models/mnm/prajna.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/prajna.checkpoint/assets\n",
      "111/111 [==============================] - 53s 476ms/step - loss: 0.2794 - acc: 0.8887 - val_loss: 0.1121 - val_acc: 0.9676\n",
      "Epoch 4/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9023\n",
      "Epoch 00004: val_loss improved from 0.11207 to 0.10567, saving model to ../models/mnm/prajna.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/prajna.checkpoint/assets\n",
      "111/111 [==============================] - 54s 489ms/step - loss: 0.2575 - acc: 0.9023 - val_loss: 0.1057 - val_acc: 0.9784\n",
      "Epoch 5/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.8849\n",
      "Epoch 00005: val_loss did not improve from 0.10567\n",
      "111/111 [==============================] - 52s 464ms/step - loss: 0.2669 - acc: 0.8860 - val_loss: 0.1176 - val_acc: 0.9676\n",
      "Epoch 6/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9105\n",
      "Epoch 00006: val_loss improved from 0.10567 to 0.08015, saving model to ../models/mnm/prajna.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/prajna.checkpoint/assets\n",
      "111/111 [==============================] - 52s 468ms/step - loss: 0.2265 - acc: 0.9113 - val_loss: 0.0802 - val_acc: 0.9856\n",
      "Epoch 7/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9269\n",
      "Epoch 00007: val_loss improved from 0.08015 to 0.04467, saving model to ../models/mnm/prajna.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/prajna.checkpoint/assets\n",
      "111/111 [==============================] - 52s 466ms/step - loss: 0.1730 - acc: 0.9267 - val_loss: 0.0447 - val_acc: 0.9820\n",
      "Epoch 8/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9434\n",
      "Epoch 00008: val_loss improved from 0.04467 to 0.04133, saving model to ../models/mnm/prajna.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/prajna.checkpoint/assets\n",
      "111/111 [==============================] - 52s 468ms/step - loss: 0.1568 - acc: 0.9439 - val_loss: 0.0413 - val_acc: 0.9820\n",
      "Epoch 9/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9562\n",
      "Epoch 00009: val_loss did not improve from 0.04133\n",
      "111/111 [==============================] - 51s 461ms/step - loss: 0.1476 - acc: 0.9566 - val_loss: 0.0571 - val_acc: 0.9784\n",
      "Epoch 10/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9525\n",
      "Epoch 00010: val_loss did not improve from 0.04133\n",
      "111/111 [==============================] - 51s 459ms/step - loss: 0.1463 - acc: 0.9511 - val_loss: 0.0533 - val_acc: 0.9748\n",
      "Epoch 11/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9553\n",
      "Epoch 00011: val_loss improved from 0.04133 to 0.02741, saving model to ../models/mnm/prajna.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/prajna.checkpoint/assets\n",
      "111/111 [==============================] - 52s 465ms/step - loss: 0.1383 - acc: 0.9557 - val_loss: 0.0274 - val_acc: 0.9928\n",
      "Epoch 12/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9580\n",
      "Epoch 00012: val_loss did not improve from 0.02741\n",
      "111/111 [==============================] - 54s 491ms/step - loss: 0.1215 - acc: 0.9584 - val_loss: 0.0373 - val_acc: 0.9856\n",
      "Epoch 13/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9434\n",
      "Epoch 00013: val_loss did not improve from 0.02741\n",
      "111/111 [==============================] - 54s 487ms/step - loss: 0.1598 - acc: 0.9439 - val_loss: 0.1247 - val_acc: 0.9460\n",
      "Epoch 14/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9470\n",
      "Epoch 00014: val_loss did not improve from 0.02741\n",
      "111/111 [==============================] - 54s 484ms/step - loss: 0.1560 - acc: 0.9466 - val_loss: 0.0609 - val_acc: 0.9892\n",
      "Epoch 15/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9342\n",
      "Epoch 00015: val_loss did not improve from 0.02741\n",
      "111/111 [==============================] - 51s 461ms/step - loss: 0.1687 - acc: 0.9348 - val_loss: 0.0435 - val_acc: 0.9820\n",
      "Epoch 16/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9662\n",
      "Epoch 00016: val_loss did not improve from 0.02741\n",
      "111/111 [==============================] - 72s 648ms/step - loss: 0.1139 - acc: 0.9656 - val_loss: 0.0449 - val_acc: 0.9784\n",
      "Epoch 17/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9607\n",
      "Epoch 00017: val_loss did not improve from 0.02741\n",
      "111/111 [==============================] - 70s 634ms/step - loss: 0.1198 - acc: 0.9611 - val_loss: 0.0427 - val_acc: 0.9820\n",
      "Epoch 18/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9735\n",
      "Epoch 00018: val_loss did not improve from 0.02741\n",
      "111/111 [==============================] - 55s 494ms/step - loss: 0.0785 - acc: 0.9738 - val_loss: 0.0565 - val_acc: 0.9712\n",
      "Epoch 19/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9498\n",
      "Epoch 00019: val_loss did not improve from 0.02741\n",
      "111/111 [==============================] - 54s 488ms/step - loss: 0.1425 - acc: 0.9493 - val_loss: 0.0375 - val_acc: 0.9928\n",
      "Epoch 20/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9662\n",
      "Epoch 00020: val_loss did not improve from 0.02741\n",
      "111/111 [==============================] - 56s 503ms/step - loss: 0.1022 - acc: 0.9665 - val_loss: 0.0685 - val_acc: 0.9676\n",
      "Epoch 21/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9580\n",
      "Epoch 00021: val_loss improved from 0.02741 to 0.02612, saving model to ../models/mnm/prajna.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/prajna.checkpoint/assets\n",
      "111/111 [==============================] - 54s 488ms/step - loss: 0.0960 - acc: 0.9575 - val_loss: 0.0261 - val_acc: 0.9892\n",
      "Epoch 22/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9589\n",
      "Epoch 00022: val_loss did not improve from 0.02612\n",
      "111/111 [==============================] - 53s 477ms/step - loss: 0.1297 - acc: 0.9593 - val_loss: 0.0270 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9753\n",
      "Epoch 00023: val_loss improved from 0.02612 to 0.01202, saving model to ../models/mnm/prajna.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/prajna.checkpoint/assets\n",
      "111/111 [==============================] - 59s 530ms/step - loss: 0.0885 - acc: 0.9756 - val_loss: 0.0120 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9680\n",
      "Epoch 00024: val_loss did not improve from 0.01202\n",
      "111/111 [==============================] - 57s 515ms/step - loss: 0.1171 - acc: 0.9683 - val_loss: 0.0824 - val_acc: 0.9532\n",
      "Epoch 25/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9589\n",
      "Epoch 00025: val_loss did not improve from 0.01202\n",
      "111/111 [==============================] - 57s 512ms/step - loss: 0.1335 - acc: 0.9584 - val_loss: 0.0342 - val_acc: 0.9928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9406\n",
      "Epoch 00026: val_loss did not improve from 0.01202\n",
      "111/111 [==============================] - 55s 499ms/step - loss: 0.1632 - acc: 0.9403 - val_loss: 0.0758 - val_acc: 0.9496\n",
      "Epoch 27/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9744\n",
      "Epoch 00027: val_loss did not improve from 0.01202\n",
      "111/111 [==============================] - 57s 516ms/step - loss: 0.0829 - acc: 0.9747 - val_loss: 0.0128 - val_acc: 0.9964\n",
      "Epoch 28/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9744\n",
      "Epoch 00028: val_loss improved from 0.01202 to 0.00946, saving model to ../models/mnm/prajna.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/prajna.checkpoint/assets\n",
      "111/111 [==============================] - 60s 542ms/step - loss: 0.0793 - acc: 0.9747 - val_loss: 0.0095 - val_acc: 0.9964\n",
      "Epoch 29/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9708\n",
      "Epoch 00029: val_loss did not improve from 0.00946\n",
      "111/111 [==============================] - 56s 507ms/step - loss: 0.0948 - acc: 0.9701 - val_loss: 0.0354 - val_acc: 0.9892\n",
      "Epoch 30/30\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9616\n",
      "Epoch 00030: val_loss did not improve from 0.00946\n",
      "111/111 [==============================] - 56s 503ms/step - loss: 0.0911 - acc: 0.9620 - val_loss: 0.0230 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=30,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Did We Do?\n",
    "\n",
    "To answer that, we have to answer a few other questions:\n",
    "1. What do the training/validation results look like?\n",
    "2. How well does our model predict a completely different dataset?\n",
    "  + For this, we'll use the training set from Kaggle so we can also check our accuracy\n",
    "3. How well does our model predict in a different setting?\n",
    "  + We'll use a live WebCam feed or video file and have our model predict in real time (or close to it -- framerate is spotty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Another Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:52:48.085910Z",
     "start_time": "2020-11-14T03:52:48.019611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>classname</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2756.png</td>\n",
       "      <td>69</td>\n",
       "      <td>126</td>\n",
       "      <td>294</td>\n",
       "      <td>392</td>\n",
       "      <td>face_with_mask</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2756.png</td>\n",
       "      <td>505</td>\n",
       "      <td>10</td>\n",
       "      <td>723</td>\n",
       "      <td>283</td>\n",
       "      <td>face_with_mask</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2756.png</td>\n",
       "      <td>75</td>\n",
       "      <td>252</td>\n",
       "      <td>264</td>\n",
       "      <td>390</td>\n",
       "      <td>mask_colorful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2756.png</td>\n",
       "      <td>521</td>\n",
       "      <td>136</td>\n",
       "      <td>711</td>\n",
       "      <td>277</td>\n",
       "      <td>mask_colorful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6098.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>85</td>\n",
       "      <td>728</td>\n",
       "      <td>653</td>\n",
       "      <td>face_no_mask</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15407</th>\n",
       "      <td>1894.jpg</td>\n",
       "      <td>437</td>\n",
       "      <td>121</td>\n",
       "      <td>907</td>\n",
       "      <td>644</td>\n",
       "      <td>face_with_mask</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15408</th>\n",
       "      <td>1894.jpg</td>\n",
       "      <td>557</td>\n",
       "      <td>363</td>\n",
       "      <td>876</td>\n",
       "      <td>636</td>\n",
       "      <td>mask_surgical</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15409</th>\n",
       "      <td>1894.jpg</td>\n",
       "      <td>411</td>\n",
       "      <td>3</td>\n",
       "      <td>940</td>\n",
       "      <td>325</td>\n",
       "      <td>hat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15410</th>\n",
       "      <td>3216.png</td>\n",
       "      <td>126</td>\n",
       "      <td>69</td>\n",
       "      <td>409</td>\n",
       "      <td>463</td>\n",
       "      <td>face_with_mask</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15411</th>\n",
       "      <td>3216.png</td>\n",
       "      <td>136</td>\n",
       "      <td>289</td>\n",
       "      <td>393</td>\n",
       "      <td>461</td>\n",
       "      <td>mask_colorful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15412 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           name    x    y    w    h       classname  mask\n",
       "0      2756.png   69  126  294  392  face_with_mask     1\n",
       "1      2756.png  505   10  723  283  face_with_mask     1\n",
       "2      2756.png   75  252  264  390   mask_colorful     1\n",
       "3      2756.png  521  136  711  277   mask_colorful     1\n",
       "4      6098.jpg  360   85  728  653    face_no_mask     0\n",
       "...         ...  ...  ...  ...  ...             ...   ...\n",
       "15407  1894.jpg  437  121  907  644  face_with_mask     1\n",
       "15408  1894.jpg  557  363  876  636   mask_surgical     1\n",
       "15409  1894.jpg  411    3  940  325             hat     0\n",
       "15410  3216.png  126   69  409  463  face_with_mask     1\n",
       "15411  3216.png  136  289  393  461   mask_colorful     1\n",
       "\n",
       "[15412 rows x 7 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see how well it predicts on another dataset\n",
    "df = pd.read_csv(os.path.join(IMG_DATA_DIR, 'kaggle_training.csv'), index_col=0)\n",
    "\n",
    "# the kaggle dataset actually had errors in its labeling -- on purpose\n",
    "df.rename(columns={'x1': 'x', 'x2': 'y', 'y1': 'w', 'y2': 'h'}, inplace=True)\n",
    "df.drop(['bbox'], axis=1, inplace=True)\n",
    "\n",
    "df['mask'] = np.where(df['classname'].isin(\n",
    "    ['face_with_mask', 'mask_surgical', 'mask_colorful']),\n",
    "    1, 0\n",
    ")\n",
    "                      \n",
    "test_images = list(df['name'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T17:22:06.654284Z",
     "start_time": "2020-11-14T17:22:04.564105Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 22500 into shape (1,150,150,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-39e6412d7eef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mresized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_region\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresized\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mreshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mreshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreshaped\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_region\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    299\u001b[0m            [5, 6]])\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 22500 into shape (1,150,150,3)"
     ]
    }
   ],
   "source": [
    "labels_dict={ 0:'No Mask', 1:'Mask' }\n",
    "color_dict={ 0: (0,0,255), 1: (0,255,0) }\n",
    "\n",
    "name = test_images[0]\n",
    "\n",
    "# fetch the image from the remote repository\n",
    "url = os.path.join(KAGGLE_IMAGES_DIR, \"{}?raw=true\".format(name))\n",
    "resp = urllib.request.urlopen(url)\n",
    "image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "image = cv2.imdecode(image, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "# image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     print(image[0, 0])\n",
    "\n",
    "# locate the image within the dataframe so we can find bounding boxes\n",
    "# NOTE: bounding boxes were pre-detected in this dataset and stored in the dataframe CSV\n",
    "to_test = df[df['name'] == name]\n",
    "#     print(to_test)\n",
    "\n",
    "\n",
    "# print([i for i in list(faces) if len(i) > 4])\n",
    "for i in range(len(to_test)):\n",
    "    im = df.iloc[i]\n",
    "#     print(im['x'])\n",
    "\n",
    "    face_region = image[im['y']:im['y']+im['h'], im['x']:im['x']+im['w']]\n",
    "#     print(face_region.shape)\n",
    "    resized = cv2.resize(face_region, (150,150))\n",
    "    normalized = resized/255.0\n",
    "    reshaped = np.reshape(normalized, (1, 150, 150, 3))\n",
    "    reshaped = np.vstack([reshaped])\n",
    "    pred = model.predict(face_region)\n",
    "    print(pred)\n",
    "        \n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:31:08.060942Z",
     "start_time": "2020-11-14T03:31:07.983804Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-c2b7463999e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv2_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_as_ubyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m69\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m126\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m294\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m392\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "cv2_image = img_as_ubyte(image[:, :, ::-1])\n",
    "gray = cv2.cvtColor(cv2_image, cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(gray)\n",
    "\n",
    "a = [69, 126, 294, 392]\n",
    "x, y, w, h = a\n",
    "len(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:23:11.287659Z",
     "start_time": "2020-11-14T03:22:34.014202Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-862ab91bf9bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# fetch the image from the remote repository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKAGGLE_IMAGES_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{}?raw=true\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# convert the RGB image into BGR then Grayscale for compatibility with OpenCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mplugin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tifffile'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/site-packages/skimage/io/util.py\u001b[0m in \u001b[0;36mfile_or_url_context\u001b[0;34m(resource_name)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamedTemporaryFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# f must be closed before yielding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 544\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1361\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for name in test_images:\n",
    "    # fetch the image from the remote repository\n",
    "    url = os.path.join(KAGGLE_IMAGES_DIR, \"{}?raw=true\".format(name))\n",
    "    image = io.imread(url)\n",
    "    \n",
    "    # convert the RGB image into BGR then Grayscale for compatibility with OpenCV\n",
    "    image = img_as_ubyte(image[:, :, ::-1])\n",
    "    image = cv2.cvtColor(image, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "#     print(image[0, 0])\n",
    "    \n",
    "    # locate the image within the dataframe so we can find bounding boxes\n",
    "    # NOTE: bounding boxes were pre-detected in this dataset and stored in the dataframe CSV\n",
    "    to_test = df[df['name'] == name]\n",
    "#     print(to_test)\n",
    "        \n",
    "    faces = to_test['bbox']\n",
    "#     print(faces)\n",
    "    for face in list(faces):\n",
    "#         print(\"\\n{}\\n\".format(face))\n",
    "#         if len(face) == 4 and image is not None:\n",
    "            print(face)\n",
    "            x, y, w, h = face\n",
    "            face_region = image[y:y+h, x:x+w]\n",
    "            resized = cv2.resize(face, (150,150))\n",
    "            normalized = resized/255.0\n",
    "            reshaped = np.reshape(normalized, (1, 150, 150, 3))\n",
    "            reshaped = np.vstack([reshaped])\n",
    "            pred = model.predict(reshaped)\n",
    "            print(pred)\n",
    "        \n",
    "\n",
    "        \n",
    "test_images    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Our Model With Video Feed\n",
    "\n",
    "+ We have two available facial detection algorithms: \n",
    "  1. Haar Cascade - faster detection, but only tuned for full, frontal views of faces.\n",
    "  2. MTCNN - slower detection, but returns extra features and can detect faces from a variety of angles -- not only the front.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T03:07:01.970708Z",
     "start_time": "2020-11-14T03:07:01.429120Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d4a4495f38d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# reverse the scale-down of the frame for bounding box (MTCNN)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'box'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;31m# print(f['keypoints'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mkeypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keypoints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# use haar cascade to detect faces (frontal)\n",
    "HC_FRONTAL_FACE = os.path.join(HC_DATA_DIR, \n",
    "                               'haarcascade_frontalface_default.xml')\n",
    "classifier = cv2.CascadeClassifier(HC_FRONTAL_FACE)\n",
    "\n",
    "\n",
    "# use MTCNN to detect faces\n",
    "# classifier = MTCNN()\n",
    "\n",
    "\n",
    "labels_dict={ 0:'No Mask', 1:'Mask' }\n",
    "color_dict={ 0: (0,0,255), 1: (0,255,0) }\n",
    "\n",
    "size = 4\n",
    "\n",
    "# open webcam feed\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# alternatively, open feed from a file\n",
    "cap = cv2.VideoCapture('/Users/brtonnies/ArtificialIntelligence/comporg.mov')\n",
    "\n",
    "# EVEN from a mobile device (UNTESTED)\n",
    "# cap = cv.VideoCapture(1)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if frame is not None:\n",
    "        # frame = cv2.flip(frame, 1, 1) # flip so view looks more natural live (?)\n",
    "        \n",
    "\n",
    "        # detection is a little faster on smaller images -- so let's make it small\n",
    "        small = cv2.resize(frame, (frame.shape[1] // size, frame.shape[0] // size))\n",
    "\n",
    "        # detect faces with haar cascades\n",
    "        faces = classifier.detectMultiScale(small)\n",
    "        \n",
    "        # detect faces with MTCNN\n",
    "#         faces = classifier.detect_faces(small)\n",
    "\n",
    "        for f in faces:\n",
    "            # reverse the scale-down of the frame for bounding box (haar)\n",
    "            (x, y, w, h) = [v * size for v in f] \n",
    "            keypoints = None\n",
    "            \n",
    "            # reverse the scale-down of the frame for bounding box (MTCNN)\n",
    "#             (x, y, w, h) = [v * size for v in f['box']]\n",
    "            # print(f['keypoints'])\n",
    "#             keypoints = f['keypoints']\n",
    "                \n",
    "        \n",
    "            face = frame[y:y+h, x:x+w]\n",
    "            resized = cv2.resize(face, (150,150))\n",
    "            normalized = resized/255.0\n",
    "            reshaped = np.reshape(normalized,(1,150,150,3))\n",
    "            reshaped = np.vstack([reshaped])\n",
    "            result = model.predict(reshaped)\n",
    "#             print(result)\n",
    "\n",
    "            label = np.argmax(result, axis=1)[0]\n",
    "#             print(\"PREDICTION: {}\".format(labels_dict[label]))\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), color_dict[label], 2)\n",
    "            cv2.rectangle(frame, (x,y-40), (x+w,y), color_dict[label], -1)\n",
    "            cv2.putText(\n",
    "                frame, \n",
    "                labels_dict[label], \n",
    "                (x, y-10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.8,\n",
    "                (255,255,255),\n",
    "                2\n",
    "            )\n",
    "            \n",
    "            # extra fun when using MTCNN :)\n",
    "            if keypoints is not None:\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['left_eye']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['right_eye']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['nose']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['mouth_left']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['mouth_right']])), \n",
    "                    2, (255,0,255), 2)\n",
    "            # end of extra fun with MTCNN\n",
    "\n",
    "        cv2.imshow('LIVE DETECTION ACTIVE', frame)\n",
    "        key = cv2.waitKey(10)\n",
    "\n",
    "        if key == 27: # if Esc key pressed -- end feed\n",
    "            break\n",
    "            \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "# stop video\n",
    "cap.release()\n",
    "\n",
    "# close windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:32:50.497371Z",
     "start_time": "2020-11-14T01:04:16.530Z"
    }
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
