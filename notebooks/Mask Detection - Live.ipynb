{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:36:42.907756Z",
     "start_time": "2020-11-15T19:36:36.706650Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# our nice list of imports is in here\n",
    "#i hate this, but the notebook won't recognize our custom modules w/o it\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import lib\n",
    "from lib import utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.models import Sequential        # keras is only compatible with python 3.6 or lower\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "\n",
    "from skimage import io, data\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.transform import rescale\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Mask Detection\n",
    "\n",
    "\n",
    "### Brief Overview\n",
    "\n",
    "Simply put, our goal is to create a neural network that will (1) identify face masks or lack thereof on a human face, and (2) determine whether that facemask is correctly worn.\n",
    "\n",
    "\n",
    "Luckily for us, Kaggle had a contest earlier this year related to (1) -- so a dataset was readily available, and it has proven to be quite effective at training a mask/no-mask neural net with our testing images. (Kaggle)\n",
    "\n",
    "\n",
    "However, the dataset is extremely lacking when it comes to incorrect mask usage.  We did locate an additional dataset that consists only of faces with correctly worn masks and incorrectly worn masks. (CMFD/IMFD)\n",
    "\n",
    "### Data\n",
    "\n",
    "1. Training Data - We have training data from both datasets.  The images in each set have been pre-processed with relevant data written to CSV files to ease the process load.\n",
    "  + Kaggle Dataset -- \n",
    "2. Testing Data - Our testing data consists of \n",
    "\n",
    "\n",
    "### Models\n",
    "\n",
    "To that end, we need a few different models:\n",
    "1. One to predict whether there's a mask on a face or no mask\n",
    "  + For this, we will use the Kaggle Dataset, and focus on the two largest features available in it: whether there is a face mask or not.\n",
    "  + Potentially, we could add the data from the CMFD/IMFD dataset as representations of masks for the training set, since we'll be determining whether they're correct or incorrectly worn in the next stage.  The only concern there is a dataset that's very, very heavy on masks vs. no masks\n",
    "2. One to predict whether, given a mask is on a face, it is worn correctly\n",
    "  + For this, we will use the CMFD/IMFD dataset.  There are only two features to this dataset: correctly worn and incorrectly worn face masks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:36:42.947382Z",
     "start_time": "2020-11-15T19:36:42.909702Z"
    }
   },
   "outputs": [],
   "source": [
    "# define directory paths for easier navigation\n",
    "NOTEBOOK_DIR = !pwd\n",
    "NOTEBOOK_DIR = NOTEBOOK_DIR[0]\n",
    "ROOT_DIR = os.path.abspath(os.path.join(NOTEBOOK_DIR, os.pardir))\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'images')\n",
    "IMG_DATA_DIR = os.path.join(DATA_DIR, 'img_data')\n",
    "KAGGLE_DIR = os.path.join(IMAGES_DIR, 'kaggle')\n",
    "HC_DATA_DIR = os.path.join(DATA_DIR, 'haar_cascades')\n",
    "\n",
    "# MNM_TRIAL_DIR = os.path.join(IMAGES_DIR, 'mnm_trial')\n",
    "# MNM_TRAINING = os.path.join(MNM_TRIAL_DIR, 'training')\n",
    "# MNM_TESTING = os.path.join(MNM_TRIAL_DIR, 'testing')\n",
    "\n",
    "PRAJNA_SRC_DIR = os.path.join(DATA_DIR, 'prajna-dataset')\n",
    "PMASK_SRC = os.path.join(PRAJNA_SRC_DIR, 'mask')\n",
    "PNOMASK_SRC = os.path.join(PRAJNA_SRC_DIR, 'nomask')\n",
    "\n",
    "PTRAINING_DIR = os.path.join(PRAJNA_SRC_DIR, 'dest/train')\n",
    "PTESTING_DIR = os.path.join(PRAJNA_SRC_DIR, 'dest/test')\n",
    "PVAL_DIR = os.path.join(PRAJNA_SRC_DIR, 'dest/val')\n",
    "\n",
    "PTRAINING_MASK = os.path.join(PRAJNA_SRC_DIR, 'dest/train/mask')\n",
    "PTESTING_MASK = os.path.join(PRAJNA_SRC_DIR, 'dest/test/mask')\n",
    "PVAL_MASK = os.path.join(PRAJNA_SRC_DIR, 'dest/val/mask')\n",
    "\n",
    "PTRAINING_NOMASK = os.path.join(PRAJNA_SRC_DIR, 'dest/train/nomask')\n",
    "PTESTING_NOMASK = os.path.join(PRAJNA_SRC_DIR, 'dest/test/nomask')\n",
    "PVAL_NOMASK = os.path.join(PRAJNA_SRC_DIR, 'dest/val/nomask')\n",
    "\n",
    "\n",
    "EXTENDED_SRC_DIR = os.path.join(DATA_DIR, 'extended-dataset')\n",
    "EMASK_SRC = os.path.join(EXTENDED_SRC_DIR, 'mask')\n",
    "ENOMASK_SRC = os.path.join(EXTENDED_SRC_DIR, 'nomask')\n",
    "\n",
    "ETRAINING_DIR = os.path.join(EXTENDED_SRC_DIR, 'dest/train')\n",
    "ETESTING_DIR = os.path.join(EXTENDED_SRC_DIR, 'dest/test')\n",
    "EVAL_DIR = os.path.join(EXTENDED_SRC_DIR, 'dest/val')\n",
    "\n",
    "ETRAINING_MASK = os.path.join(EXTENDED_SRC_DIR, 'dest/train/mask')\n",
    "ETESTING_MASK = os.path.join(EXTENDED_SRC_DIR, 'dest/test/mask')\n",
    "EVAL_MASK = os.path.join(EXTENDED_SRC_DIR, 'dest/val/mask')\n",
    "\n",
    "ETRAINING_NOMASK = os.path.join(EXTENDED_SRC_DIR, 'dest/train/nomask')\n",
    "ETESTING_NOMASK = os.path.join(EXTENDED_SRC_DIR, 'dest/test/nomask')\n",
    "EVAL_NOMASK = os.path.join(EXTENDED_SRC_DIR, 'dest/val/nomask')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MASK_SRC_DIR=os.path.join(MNM_TRIAL_DIR, 'mask')\n",
    "# TRAINING_MASK=os.path.join(MNM_TRAINING, 'mask')\n",
    "# TESTING_MASK=os.path.join(MNM_TESTING, 'mask')\n",
    "\n",
    "# NOMASK_SRC_DIR=os.path.join(MNM_TRIAL_DIR, 'nomask')\n",
    "# TRAINING_NOMASK=os.path.join(MNM_TRAINING, 'nomask')\n",
    "# TESTING_NOMASK=os.path.join(MNM_TESTING, 'nomask')\n",
    "\n",
    "KAGGLE_IMAGES_DIR = \"https://github.com/brtonnies/face-mask-detection/blob/data-branch/data/images\"\n",
    "# CMFD_IMAGES_DIR = \"https://github.com/brtonnies/face-mask-detection/blob/data-branch/data/images/CMFD/images\"\n",
    "# IMFD_IMAGES_DIR = \"https://github.com/brtonnies/face-mask-detection/blob/data-branch/data/images/IMFD/images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Mask vs. No Mask (MNM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:06.319969Z",
     "start_time": "2020-11-12T20:23:06.317684Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:06.660624Z",
     "start_time": "2020-11-12T20:23:06.658847Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparations\n",
    "\n",
    "+ We keep pre-compiled versions of our different models on hand (again, to reduce process/load times)\n",
    "+ Unless the training data changes, we default to using those existing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:08.127649Z",
     "start_time": "2020-11-12T20:23:08.125432Z"
    }
   },
   "source": [
    "1. First model used ONLY Prajna Dataset (97% acc)\n",
    "2. Second model is training now, with additional 'no mask' images from Kaggle\n",
    "3. Third will have additional 'mask' images from kaggle\n",
    "\n",
    "**DON'T FORGET TO REVERT IMAGES TO BEST MODEL VERSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:08.590623Z",
     "start_time": "2020-11-12T20:23:08.588383Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:36:42.952748Z",
     "start_time": "2020-11-15T19:36:42.950236Z"
    }
   },
   "outputs": [],
   "source": [
    "# def data_summary(main_path):\n",
    "    \n",
    "#     yes_path = main_path+'/mask'\n",
    "#     no_path = main_path+'/nomask'\n",
    "        \n",
    "#     # number of files (images) that are in the the folder named 'yes' that represent tumorous (positive) examples\n",
    "#     m_pos = len(os.listdir(yes_path))\n",
    "#     # number of files (images) that are in the the folder named 'no' that represent non-tumorous (negative) examples\n",
    "#     m_neg = len(os.listdir(no_path))\n",
    "#     # number of all examples\n",
    "#     m = (m_pos+m_neg)\n",
    "    \n",
    "#     pos_prec = (m_pos* 100.0)/ m\n",
    "#     neg_prec = (m_neg* 100.0)/ m\n",
    "    \n",
    "#     print(f\"Number of examples: {m}\")\n",
    "#     print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\") \n",
    "#     print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\") \n",
    "    \n",
    "# augmented_data_path = os.path.join(EXTENDED_SRC_DIR, 'dest')\n",
    "# data_summary(augmented_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:36:46.203982Z",
     "start_time": "2020-11-15T19:36:42.955278Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(src, train, test, split_size):\n",
    "    dataset = []\n",
    "    \n",
    "    for unitData in os.listdir(src):\n",
    "        data = os.path.join(src, unitData)\n",
    "        if(os.path.getsize(data) > 0 and os.path.isfile(data)):\n",
    "            dataset.append(unitData)\n",
    "        else:\n",
    "            print('Skipped ' + unitData)\n",
    "            print('Invalid file i.e zero size')\n",
    "    \n",
    "    train_set_length = int(len(dataset) * split_size)\n",
    "    test_set_length = int(len(dataset) - train_set_length)\n",
    "    shuffled_set = random.sample(dataset, len(dataset))\n",
    "    train_set = dataset[0:train_set_length]\n",
    "    test_set = dataset[-test_set_length:]\n",
    "       \n",
    "    for unitData in train_set:\n",
    "        temp_train_set = os.path.join(src, unitData)\n",
    "        final_train_set = os.path.join(train, unitData)\n",
    "        \n",
    "        copyfile(temp_train_set, final_train_set)\n",
    "    \n",
    "    for unitData in test_set:\n",
    "        temp_test_set = os.path.join(src, unitData)\n",
    "        final_test_set = os.path.join(test, unitData)\n",
    "        copyfile(temp_test_set, final_test_set)\n",
    "\n",
    "        \n",
    "split_size = 0.8\n",
    "split_data(EMASK_SRC, ETRAINING_MASK, ETESTING_MASK, split_size)\n",
    "split_data(ENOMASK_SRC, ETRAINING_NOMASK, ETESTING_NOMASK, split_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:11.386097Z",
     "start_time": "2020-11-12T20:23:11.384123Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:23:11.807979Z",
     "start_time": "2020-11-12T20:23:11.805661Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-11T19:57:01.408Z"
    }
   },
   "source": [
    "## Keras Sequential Model\n",
    "\n",
    "#### Building the Model\n",
    "In this proposed method, the Face Mask detection model is built using the Sequential API of the keras library. This allows us to create the new layers for our model step by step. The various layers used for our CNN model is described below.\n",
    "\n",
    "1. The first layer is Conv2D with **100 kernels** of size $3\\times3$. \n",
    "  + The activation function is ‘ReLu’.  \n",
    "  + The input size is set to $150\\times 150 \\times 3$ for all the images to be trained and tested\n",
    "\n",
    "2. MaxPooling2D is used for the second layer with pool size of $2 \\times 2$.\n",
    "\n",
    "3. The next layer is another Conv2D layer \n",
    "  + Has its own 100 kernels of the same size: $3\\times3$ \n",
    "  + Activation function is ‘ReLu’. \n",
    "  + The layer is followed by another MaxPooling2D layer with pool size $2 \\times 2$.\n",
    "\n",
    "4. Then the Flatten() layer to flatten *all* the layers into a single dimension.\n",
    "\n",
    "5. **NEW:** After the Flatten layer, we added dropout (0.5) layer to prevent the model from overfitting -- which was killing us in early trials.\n",
    "\n",
    "6. Then we used the Dense layer \n",
    "  + 50 'units' \n",
    "  + Activation function: ‘ReLu’.\n",
    "\n",
    "7. The last layer of our model will be another Dense Layer, with only two units and activation function ‘Softmax’. The softmax function outputs a vector representing the probability distributions of each of the input units. Here, two input units are used. The softmax function will output a vector with two probability distribution values: is there a mask on this face or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:37:11.702992Z",
     "start_time": "2020-11-15T19:37:11.460357Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "kernel_size = (3, 3)\n",
    "image_shape = (150, 150, 3)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(100, kernel_size, activation='tanh', input_shape=image_shape, strides=2),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "#     tf.keras.layers.Conv2D(100, (3,3), activation='tanh'),\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(100, kernel_size, activation='tanh'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.2),   # previously we were way overfitting, so we added dropout\n",
    "    tf.keras.layers.Dense(50, activation='tanh'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax'),\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-4, epsilon=1e-2)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Training/Testing/Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:39:00.097649Z",
     "start_time": "2020-11-15T19:38:59.253942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1680 images belonging to 2 classes.\n",
      "Found 421 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(ETRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(150, 150))\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(ETESTING_DIR, \n",
    "                                                              batch_size=10, \n",
    "                                                              target_size=(150, 150))\n",
    "\n",
    "# (1) train the all data extended model\n",
    "# (2) re-train prajna, prajna-kaggle-mask to save new checkpoints (and load them :D)\n",
    "# (3) also try and do one model where only new masks are added to the set\n",
    "\n",
    "# model_path = '../models/mnm/prajna'\n",
    "# checkpoint_path = '../models/mnm/prajna.checkpoint' \n",
    "\n",
    "model_path = '../models/mnm/extended_dataset_tanh'\n",
    "checkpoint_path = '../models/mnm/extended_datasest_tanh.checkpoint'\n",
    "\n",
    "# model_path = '../models/mnm/prajna-kaggle-nomask-extend'\n",
    "# checkpoint_path = '../models/mnm/prajna-kaggle-nomask-extend-{epoch:03d}.model'\n",
    "# model_path = '../models/mnm/prajna-kaggle-mask-extend'\n",
    "# checkpoint_path = '../models/mnm/prajna-kaggle-mask-extend-{epoch:03d}.model'\n",
    "# model_path = '../models/mnm/prajna-kaggle-all-extend'\n",
    "# checkpoint_path = '../models/mnm/prajna-kaggle-all-extend-{epoch:03d}.model'\n",
    "\n",
    "\n",
    "# NOTE: here's something cool - we can use checkpoints\n",
    "#       to save/load the weights from previous model training\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     model.load_weights(os.path.join(checkpoint_path, 'variables/variables'))\n",
    "    \n",
    "# alternatively, we can load the whole model from the checkpoint state\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model = load_model(checkpoint_path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path, \n",
    "    monitor = 'val_loss', \n",
    "    verbose = 1, \n",
    "    save_best_only = True,\n",
    "    mode = 'auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-15T23:26:09.509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9180\n",
      "Epoch 00001: val_loss improved from inf to 0.12802, saving model to ../models/mnm/extended_datasest_tanh.checkpoint\n",
      "WARNING:tensorflow:From /Users/brtonnies/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/extended_datasest_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 178s 1s/step - loss: 0.2209 - acc: 0.9179 - val_loss: 0.1280 - val_acc: 0.9501\n",
      "Epoch 2/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9228\n",
      "Epoch 00002: val_loss did not improve from 0.12802\n",
      "168/168 [==============================] - 148s 879ms/step - loss: 0.2067 - acc: 0.9232 - val_loss: 0.1303 - val_acc: 0.9549\n",
      "Epoch 3/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9054\n",
      "Epoch 00003: val_loss improved from 0.12802 to 0.11547, saving model to ../models/mnm/extended_datasest_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/extended_datasest_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 154s 914ms/step - loss: 0.2290 - acc: 0.9060 - val_loss: 0.1155 - val_acc: 0.9667\n",
      "Epoch 4/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9210\n",
      "Epoch 00004: val_loss improved from 0.11547 to 0.10835, saving model to ../models/mnm/extended_datasest_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/extended_datasest_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 151s 902ms/step - loss: 0.2135 - acc: 0.9208 - val_loss: 0.1084 - val_acc: 0.9620\n",
      "Epoch 5/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9222\n",
      "Epoch 00005: val_loss did not improve from 0.10835\n",
      "168/168 [==============================] - 149s 888ms/step - loss: 0.1950 - acc: 0.9214 - val_loss: 0.1231 - val_acc: 0.9596\n",
      "Epoch 6/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9287\n",
      "Epoch 00006: val_loss did not improve from 0.10835\n",
      "168/168 [==============================] - 146s 870ms/step - loss: 0.1876 - acc: 0.9286 - val_loss: 0.1435 - val_acc: 0.9477\n",
      "Epoch 7/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9210\n",
      "Epoch 00007: val_loss improved from 0.10835 to 0.09918, saving model to ../models/mnm/extended_datasest_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/extended_datasest_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 159s 947ms/step - loss: 0.1980 - acc: 0.9208 - val_loss: 0.0992 - val_acc: 0.9715\n",
      "Epoch 8/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9275\n",
      "Epoch 00008: val_loss did not improve from 0.09918\n",
      "168/168 [==============================] - 143s 854ms/step - loss: 0.1992 - acc: 0.9274 - val_loss: 0.1322 - val_acc: 0.9477\n",
      "Epoch 9/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9275\n",
      "Epoch 00009: val_loss did not improve from 0.09918\n",
      "168/168 [==============================] - 107s 639ms/step - loss: 0.1955 - acc: 0.9280 - val_loss: 0.1165 - val_acc: 0.9525\n",
      "Epoch 10/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9311\n",
      "Epoch 00010: val_loss improved from 0.09918 to 0.09731, saving model to ../models/mnm/extended_datasest_tanh.checkpoint\n",
      "INFO:tensorflow:Assets written to: ../models/mnm/extended_datasest_tanh.checkpoint/assets\n",
      "168/168 [==============================] - 107s 636ms/step - loss: 0.1879 - acc: 0.9315 - val_loss: 0.0973 - val_acc: 0.9644\n",
      "Epoch 11/30\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 0.9198\n",
      "Epoch 00011: val_loss did not improve from 0.09731\n",
      "168/168 [==============================] - 112s 664ms/step - loss: 0.2075 - acc: 0.9196 - val_loss: 0.1933 - val_acc: 0.9287\n",
      "Epoch 12/30\n",
      "117/168 [===================>..........] - ETA: 29s - loss: 0.2075 - acc: 0.9256"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=30,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])\n",
    "\n",
    "# model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Did We Do?\n",
    "\n",
    "To answer that, we have to answer a few other questions:\n",
    "1. What do the training/validation results look like?\n",
    "  + Can we accurately predict the data in the dataset source images?\n",
    "2. How well does our model predict a completely different dataset?\n",
    "  + For this, we'll use the training set from Kaggle so we can also check our accuracy\n",
    "3. How well does our model predict in a different setting?\n",
    "  + We'll use a live WebCam feed or video file and have our model predict in real time (or close to it -- framerate is spotty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-15T23:26:28.310Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_accuracy_loss(history):\n",
    "    \"\"\"\n",
    "        Plot the accuracy and the loss during the training of the nn.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(18,8))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(221)\n",
    "    plt.plot(history.history['acc'],'bo--', label = \"acc\")\n",
    "    plt.plot(history.history['val_acc'], 'ro--', label = \"val_acc\")\n",
    "    plt.title(\"train_acc vs val_acc\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss function\n",
    "    plt.subplot(222)\n",
    "    plt.plot(history.history['loss'],'bo--', label = \"loss\")\n",
    "    plt.plot(history.history['val_loss'], 'ro--', label = \"val_loss\")\n",
    "    plt.title(\"train_loss vs val_loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_accuracy_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting The Current Dataset\n",
    "\n",
    "+ We'll start with the images in the source directory of masks\n",
    "  1. First we go through the images, use facial detection to find bounding boxes and/or keypoints\n",
    "  2. Then we  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:37:03.895718Z",
     "start_time": "2020-11-15T19:36:36.782Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(os.listdir(PMASK_SRC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Another Dataset\n",
    "\n",
    "Our model's performance in the training/validation stage was admirable.  Let's give it some images from another dataset and see how it performs with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:25:19.696878Z",
     "start_time": "2020-11-15T19:25:19.684389Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:25:19.705140Z",
     "start_time": "2020-11-15T19:25:19.700273Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:25:21.077528Z",
     "start_time": "2020-11-15T19:25:19.708263Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T18:31:39.456963Z",
     "start_time": "2020-11-14T18:31:34.189Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Our Model With Video Feed\n",
    "\n",
    "+ We have two available facial detection algorithms: \n",
    "  1. Haar Cascade - faster detection, but only tuned for full, frontal views of faces.\n",
    "  2. MTCNN - slower detection, but returns extra features and can detect faces from a variety of angles -- not only the front.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:40:11.636763Z",
     "start_time": "2020-11-15T19:39:05.180183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02698364 0.97301644]]\n",
      "[[0.02821324 0.97178674]]\n",
      "[[0.03138226 0.9686177 ]]\n",
      "[[0.03219345 0.9678065 ]]\n",
      "[[0.05055654 0.94944346]]\n",
      "[[0.03322263 0.9667773 ]]\n",
      "[[0.0358761  0.96412396]]\n",
      "[[0.05574975 0.9442503 ]]\n",
      "[[0.04656427 0.9534358 ]]\n",
      "[[0.05139342 0.9486066 ]]\n",
      "[[0.03512964 0.9648704 ]]\n",
      "[[0.05186348 0.9481365 ]]\n",
      "[[0.05911771 0.94088227]]\n",
      "[[0.04429787 0.9557022 ]]\n",
      "[[0.04275751 0.9572425 ]]\n",
      "[[0.06047365 0.9395264 ]]\n",
      "[[0.0537957 0.9462043]]\n",
      "[[0.03699295 0.96300703]]\n",
      "[[0.04701956 0.9529804 ]]\n",
      "[[0.03803528 0.96196467]]\n",
      "[[0.04771735 0.9522826 ]]\n",
      "[[0.05218094 0.94781905]]\n",
      "[[0.02474632 0.97525364]]\n",
      "[[0.04847281 0.95152724]]\n",
      "[[0.04528978 0.95471025]]\n",
      "[[0.03953552 0.9604644 ]]\n",
      "[[0.03438353 0.9656165 ]]\n",
      "[[0.05411624 0.94588375]]\n",
      "[[0.03271981 0.9672802 ]]\n",
      "[[0.03833314 0.96166694]]\n",
      "[[0.03516569 0.9648343 ]]\n",
      "[[0.05867791 0.94132215]]\n",
      "[[0.03044856 0.9695514 ]]\n",
      "[[0.03661599 0.963384  ]]\n",
      "[[0.02883824 0.9711618 ]]\n",
      "[[0.06016158 0.93983847]]\n",
      "[[0.06391361 0.9360864 ]]\n",
      "[[0.0630969 0.9369031]]\n",
      "[[0.03842336 0.96157664]]\n",
      "[[0.03052147 0.96947855]]\n",
      "[[0.04195283 0.95804715]]\n",
      "[[0.06628571 0.93371433]]\n",
      "[[0.0603449  0.93965507]]\n",
      "[[0.06502674 0.93497324]]\n",
      "[[0.05647419 0.94352585]]\n",
      "[[0.06759576 0.9324043 ]]\n",
      "[[0.04737283 0.9526272 ]]\n",
      "[[0.01299792 0.98700213]]\n",
      "[[0.01920194 0.98079807]]\n",
      "[[0.02348876 0.9765112 ]]\n",
      "[[0.04885324 0.9511467 ]]\n",
      "[[0.01353101 0.98646903]]\n",
      "[[0.02643363 0.97356635]]\n",
      "[[0.00360201 0.996398  ]]\n",
      "[[0.05107467 0.9489253 ]]\n",
      "[[0.01732207 0.982678  ]]\n",
      "[[0.06553374 0.93446624]]\n",
      "[[0.05553614 0.94446385]]\n",
      "[[0.00348133 0.9965186 ]]\n",
      "[[0.04772309 0.9522769 ]]\n",
      "[[0.03999195 0.9600081 ]]\n",
      "[[0.02592847 0.97407144]]\n",
      "[[0.0341874 0.9658126]]\n",
      "[[0.05607102 0.94392896]]\n",
      "[[0.03914133 0.96085864]]\n",
      "[[0.02697953 0.9730205 ]]\n",
      "[[0.00449255 0.9955075 ]]\n",
      "[[0.01049011 0.98950994]]\n",
      "[[0.04880193 0.95119804]]\n",
      "[[0.03181763 0.9681824 ]]\n",
      "[[0.02528352 0.9747165 ]]\n",
      "[[0.02533153 0.9746685 ]]\n",
      "[[0.03358706 0.9664129 ]]\n",
      "[[0.02876638 0.97123367]]\n",
      "[[0.00821428 0.9917857 ]]\n",
      "[[0.02924201 0.97075796]]\n",
      "[[0.0410997 0.9589003]]\n",
      "[[0.05507443 0.9449256 ]]\n",
      "[[0.05159905 0.9484009 ]]\n",
      "[[0.06809399 0.93190604]]\n",
      "[[0.05961489 0.94038516]]\n",
      "[[0.03033003 0.96966994]]\n",
      "[[0.04878787 0.9512121 ]]\n",
      "[[0.029038   0.97096205]]\n",
      "[[0.01051188 0.9894881 ]]\n",
      "[[0.04880644 0.9511935 ]]\n",
      "[[0.03990139 0.9600986 ]]\n",
      "[[0.02449579 0.97550416]]\n",
      "[[0.02751333 0.9724867 ]]\n",
      "[[0.03329474 0.96670526]]\n",
      "[[0.05140202 0.94859797]]\n",
      "[[0.05944432 0.9405557 ]]\n",
      "[[0.06337772 0.93662226]]\n",
      "[[0.02692101 0.97307897]]\n",
      "[[0.0291212 0.9708788]]\n",
      "[[0.03650094 0.96349907]]\n",
      "[[0.03169139 0.96830857]]\n",
      "[[0.04566582 0.9543342 ]]\n",
      "[[0.01882423 0.9811757 ]]\n",
      "[[0.03670816 0.9632918 ]]\n",
      "[[0.04954654 0.95045346]]\n",
      "[[0.00561273 0.99438727]]\n",
      "[[0.05248438 0.94751567]]\n",
      "[[0.04384004 0.95615995]]\n",
      "[[0.05498825 0.94501173]]\n",
      "[[0.02056261 0.9794374 ]]\n",
      "[[0.0281434  0.97185653]]\n",
      "[[0.04611111 0.95388883]]\n",
      "[[0.02548814 0.97451186]]\n",
      "[[0.03075634 0.96924365]]\n",
      "[[0.0570312 0.9429688]]\n",
      "[[0.03821356 0.96178645]]\n",
      "[[0.04577804 0.95422196]]\n",
      "[[0.02873856 0.97126144]]\n",
      "[[0.02630397 0.973696  ]]\n",
      "[[0.02832764 0.9716724 ]]\n",
      "[[0.02514526 0.9748547 ]]\n",
      "[[0.02160021 0.9783998 ]]\n",
      "[[0.02793997 0.9720601 ]]\n",
      "[[0.03237822 0.96762174]]\n",
      "[[0.0379422  0.96205777]]\n",
      "[[0.03083596 0.969164  ]]\n",
      "[[0.03580673 0.9641933 ]]\n",
      "[[0.01300354 0.9869964 ]]\n",
      "[[0.03138066 0.9686194 ]]\n",
      "[[0.02059733 0.97940266]]\n",
      "[[0.04529706 0.9547029 ]]\n",
      "[[0.05991786 0.9400822 ]]\n",
      "[[0.07113375 0.92886627]]\n",
      "[[0.05929838 0.94070166]]\n",
      "[[0.04978766 0.9502124 ]]\n",
      "[[0.05368199 0.9463181 ]]\n",
      "[[0.03355104 0.96644896]]\n",
      "[[0.03153995 0.96846   ]]\n",
      "[[0.03070309 0.9692969 ]]\n",
      "[[0.03726015 0.9627398 ]]\n",
      "[[0.03973981 0.96026015]]\n",
      "[[0.0343232 0.9656768]]\n",
      "[[0.03997865 0.9600214 ]]\n",
      "[[0.05892759 0.9410724 ]]\n",
      "[[0.06071872 0.9392812 ]]\n",
      "[[0.01588549 0.9841145 ]]\n",
      "[[0.05494064 0.9450594 ]]\n",
      "[[0.04520199 0.95479804]]\n",
      "[[0.05896101 0.941039  ]]\n",
      "[[0.03514978 0.9648503 ]]\n",
      "[[0.02599316 0.9740069 ]]\n",
      "[[0.0610544  0.93894565]]\n",
      "[[0.05576534 0.94423467]]\n",
      "[[0.06398711 0.93601286]]\n",
      "[[0.06944664 0.9305534 ]]\n",
      "[[0.0667802 0.9332198]]\n",
      "[[0.03736338 0.96263665]]\n",
      "[[0.01965961 0.9803404 ]]\n",
      "[[0.01450948 0.98549056]]\n",
      "[[0.01775329 0.9822467 ]]\n",
      "[[0.00973053 0.9902694 ]]\n",
      "[[0.01939066 0.98060936]]\n",
      "[[0.01024342 0.98975664]]\n",
      "[[0.00473868 0.9952614 ]]\n",
      "[[0.0258023  0.97419775]]\n",
      "[[0.01847685 0.98152316]]\n",
      "[[0.01160856 0.98839146]]\n",
      "[[0.00769689 0.99230313]]\n",
      "[[0.02561046 0.9743895 ]]\n",
      "[[0.03806488 0.9619351 ]]\n",
      "[[0.02292771 0.9770723 ]]\n",
      "[[0.01959697 0.98040307]]\n",
      "[[0.04698317 0.9530169 ]]\n",
      "[[0.02199771 0.97800225]]\n",
      "[[0.02714785 0.9728521 ]]\n",
      "[[0.01870281 0.9812972 ]]\n",
      "[[0.03568106 0.964319  ]]\n",
      "[[0.03021767 0.9697823 ]]\n",
      "[[0.02812482 0.97187513]]\n",
      "[[0.043981   0.95601904]]\n",
      "[[0.0648016 0.9351984]]\n",
      "[[0.03776846 0.9622315 ]]\n",
      "[[0.05668673 0.94331336]]\n",
      "[[0.02634538 0.9736546 ]]\n",
      "[[0.02219796 0.9778021 ]]\n",
      "[[0.03685978 0.9631402 ]]\n",
      "[[0.02978154 0.9702185 ]]\n",
      "[[0.06018069 0.9398193 ]]\n",
      "[[0.1085007 0.8914993]]\n",
      "[[0.33271322 0.66728675]]\n",
      "[[0.16647671 0.8335234 ]]\n",
      "[[0.28915566 0.71084434]]\n",
      "[[0.12372077 0.8762793 ]]\n",
      "[[0.10238355 0.89761645]]\n",
      "[[0.11992271 0.8800773 ]]\n",
      "[[0.17297846 0.82702154]]\n",
      "[[0.10969683 0.89030313]]\n",
      "[[0.09343185 0.9065681 ]]\n",
      "[[0.09514958 0.90485036]]\n",
      "[[0.06531278 0.9346872 ]]\n",
      "[[0.07676405 0.923236  ]]\n",
      "[[0.08305486 0.9169451 ]]\n",
      "[[0.07368723 0.9263128 ]]\n",
      "[[0.07870135 0.92129874]]\n",
      "[[0.07362984 0.9263702 ]]\n",
      "[[0.07611013 0.92388994]]\n",
      "[[0.08191723 0.91808283]]\n",
      "[[0.03904981 0.9609502 ]]\n",
      "[[0.05829798 0.941702  ]]\n",
      "[[0.04164302 0.958357  ]]\n",
      "[[0.05987579 0.9401242 ]]\n",
      "[[0.04437245 0.9556275 ]]\n",
      "[[0.05869061 0.94130945]]\n",
      "[[0.05095899 0.94904107]]\n",
      "[[0.07077789 0.9292221 ]]\n",
      "[[0.05749848 0.9425016 ]]\n",
      "[[0.05599066 0.94400936]]\n",
      "[[0.04717457 0.95282537]]\n",
      "[[0.05610773 0.94389224]]\n",
      "[[0.05015728 0.9498427 ]]\n",
      "[[0.0453133  0.95468676]]\n",
      "[[0.04349744 0.9565026 ]]\n",
      "[[0.05217975 0.94782025]]\n",
      "[[0.04368471 0.9563153 ]]\n",
      "[[0.05443535 0.9455647 ]]\n",
      "[[0.05211625 0.9478837 ]]\n",
      "[[0.06015156 0.9398485 ]]\n",
      "[[0.05099957 0.9490005 ]]\n",
      "[[0.06148477 0.93851525]]\n",
      "[[0.05886201 0.941138  ]]\n",
      "[[0.052374 0.947626]]\n",
      "[[0.06098114 0.9390188 ]]\n",
      "[[0.0646278  0.93537223]]\n",
      "[[0.05872376 0.9412763 ]]\n",
      "[[0.05587509 0.9441249 ]]\n",
      "[[0.06228005 0.93771994]]\n",
      "[[0.07066927 0.9293307 ]]\n",
      "[[0.05529215 0.9447079 ]]\n",
      "[[0.05336265 0.9466374 ]]\n",
      "[[0.04679758 0.95320237]]\n",
      "[[0.05539728 0.9446028 ]]\n",
      "[[0.05356985 0.94643015]]\n",
      "[[0.06164373 0.9383562 ]]\n",
      "[[0.06343058 0.93656945]]\n",
      "[[0.07154463 0.92845535]]\n",
      "[[0.04771356 0.9522864 ]]\n",
      "[[0.05069669 0.9493033 ]]\n",
      "[[0.06384246 0.9361575 ]]\n",
      "[[0.05651538 0.94348466]]\n",
      "[[0.04839633 0.95160365]]\n",
      "[[0.04515311 0.9548469 ]]\n"
     ]
    }
   ],
   "source": [
    "# use haar cascade to detect faces (frontal)\n",
    "# HC_FRONTAL_FACE = os.path.join(HC_DATA_DIR, \n",
    "#                                'haarcascade_frontalface_default.xml')\n",
    "# classifier = cv2.CascadeClassifier(HC_FRONTAL_FACE)\n",
    "\n",
    "\n",
    "# use MTCNN to detect faces\n",
    "classifier = MTCNN()\n",
    "\n",
    "\n",
    "labels_dict={ 0:'No Mask', 1:'Mask' }\n",
    "color_dict={ 0: (0,0,255), 1: (0,255,0) }\n",
    "\n",
    "size = 4\n",
    "\n",
    "# open webcam feed\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# alternatively, open feed from a file\n",
    "cap = cv2.VideoCapture('/Users/brtonnies/ArtificialIntelligence/memask4.mov')\n",
    "\n",
    "# EVEN from a mobile device (UNTESTED)\n",
    "# cap = cv.VideoCapture(1)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if frame is not None:\n",
    "        # frame = cv2.flip(frame, 1, 1) # flip so view looks more natural live (?)\n",
    "        \n",
    "\n",
    "        # detection is a little faster on smaller images -- so let's make it small\n",
    "        small = cv2.resize(frame, (frame.shape[1] // size, frame.shape[0] // size))\n",
    "\n",
    "        # detect faces with haar cascades\n",
    "        # faces = classifier.detectMultiScale(small)\n",
    "        \n",
    "        # detect faces with MTCNN\n",
    "        faces = classifier.detect_faces(small)\n",
    "\n",
    "        for f in faces:\n",
    "            # reverse the scale-down of the frame for bounding box (haar)\n",
    "#             (x, y, w, h) = [v * size for vstack in f] \n",
    "#             keypoints = None\n",
    "            \n",
    "            # reverse the scale-down of the frame for bounding box (MTCNN)\n",
    "            (x, y, w, h) = [v * size for v in f['box']]\n",
    "            # print(f['keypoints'])\n",
    "            keypoints = f['keypoints']\n",
    "                \n",
    "        \n",
    "            face = frame[y:y+h, x:x+w]\n",
    "            resized = cv2.resize(face, (150,150))\n",
    "            normalized = resized/255.0\n",
    "            reshaped = np.reshape(normalized,(1,150,150,3))\n",
    "            reshaped = np.vstack([reshaped])\n",
    "            result = model.predict(reshaped)\n",
    "            print(result)\n",
    "\n",
    "            label = np.argmax(result, axis=1)[0]\n",
    "#             print(\"PREDICTION: {}\".format(labels_dict[label]))\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), color_dict[label], 2)\n",
    "            cv2.rectangle(frame, (x,y-40), (x+w,y), color_dict[label], -1)\n",
    "            cv2.putText(\n",
    "                frame, \n",
    "                labels_dict[label], \n",
    "                (x, y-10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.8,\n",
    "                (255,255,255),\n",
    "                2\n",
    "            )\n",
    "            \n",
    "            # extra fun when using MTCNN :)\n",
    "            if keypoints is not None:\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['left_eye']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['right_eye']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['nose']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['mouth_left']])), \n",
    "                    2, (255,0,255), 2\n",
    "                )\n",
    "                cv2.circle(\n",
    "                    frame, \n",
    "                    (tuple([k * size for k in keypoints['mouth_right']])), \n",
    "                    2, (255,0,255), 2)\n",
    "            # end of extra fun with MTCNN\n",
    "\n",
    "        cv2.imshow('LIVE DETECTION ACTIVE', frame)\n",
    "        key = cv2.waitKey(10)\n",
    "\n",
    "        if key == 27: # if Esc key pressed -- end feed\n",
    "            break\n",
    "        \n",
    "#         clear_output(wait=True)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "# stop video\n",
    "cap.release()\n",
    "\n",
    "# close windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:37:03.898279Z",
     "start_time": "2020-11-15T19:36:36.806Z"
    }
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "563.736px",
    "left": "1135.07px",
    "right": "20px",
    "top": "77.9858px",
    "width": "442.983px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
